{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyP18F9ZhKGrXXP9MO9T/gvx",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/csch7/CSCI-4170/blob/main/Homework-04/RNNs.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "From the data, it seems like measurements were taken during 3 different continuous sessions, which happen to be pretty good sizes for training, validation, and test sets."
      ],
      "metadata": {
        "id": "3OFnXKvUPpkK"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "R9icJg6o8wgw",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c220d5af-aa7b-4624-d426-e293c7b71b37"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "# import contractions\n",
        "import re\n",
        "import torch\n",
        "import matplotlib.pyplot as plt\n",
        "from google.colab import drive\n",
        "\n",
        "drive.mount('/content/drive')\n",
        "df = pd.read_csv(\"/content/drive/MyDrive/Colab Notebooks/YoutubeCommentsDataSet.csv\")\n",
        "\n",
        "# Used for balancing the dataset\n",
        "def load_dataset(dat, tar):\n",
        "  vals, cts = np.unique(tar, return_counts=True)\n",
        "  min_val = vals[np.argmin(cts)]\n",
        "  min_ct = min(cts)\n",
        "  new_dat = dat[tar == min_val]\n",
        "  new_tar = tar[tar == min_val]\n",
        "  for i, v in enumerate(vals):\n",
        "    if v != min_val:\n",
        "      indices = list(np.where(tar == v))[0]\n",
        "      new_ind = np.random.choice(indices, min_ct)\n",
        "      new_dat = np.append(new_dat, dat[new_ind], axis=0)\n",
        "      new_tar = np.append(new_tar, tar[new_ind])\n",
        "  return new_dat, new_tar\n",
        "\n",
        "  # Function to clean text for sentiment analysis\n",
        "def clean_text(text):\n",
        "    text = str(text).lower() # Convert to lowercase\n",
        "    # text = contractions.fix(text) # Expand contractions\n",
        "    text = re.sub(r\"http\\S+|www\\.\\S+\", \"\", text) # Remove URLs starting with http/https or www\n",
        "    text = re.sub(r\"@\\w+\", \"\", text)  # Remove mentions (@usernames) to avoid irrelevant tokens\n",
        "    text = re.sub(r\"[^a-z0-9' ]\", \"\", text) # Remove special characters\n",
        "    text = re.sub(r\"\\s+\", \" \", text).strip() # Remove extra spaces\n",
        "    return text\n",
        "\n",
        "# Apply text cleaning function to the \"Comment\" column before model training\n",
        "df[\"Cleaned_Text\"] = df[\"Comment\"].apply(clean_text)\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(df.count(axis=0))\n",
        "df.dropna(axis=0, inplace = True)\n",
        "print(df.count(axis=0))\n",
        "df = df.drop_duplicates(subset = \"Comment\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DRDyHtn7tDZ7",
        "outputId": "15b685b0-a21e-40be-fb0c-928289a4f4e3"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Comment         18364\n",
            "Sentiment       18408\n",
            "Cleaned_Text    18408\n",
            "dtype: int64\n",
            "Comment         18364\n",
            "Sentiment       18364\n",
            "Cleaned_Text    18364\n",
            "dtype: int64\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "RIbssR4kUl1O"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "ys = df['Sentiment']\n",
        "df.pop('Sentiment')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 458
        },
        "id": "L9pmhrKttI7f",
        "outputId": "199e28cf-c711-4d8d-9b76-e34cea3ff57d"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0         neutral\n",
              "1        negative\n",
              "2        positive\n",
              "3        negative\n",
              "4        positive\n",
              "           ...   \n",
              "18403    positive\n",
              "18404    positive\n",
              "18405     neutral\n",
              "18406    positive\n",
              "18407    positive\n",
              "Name: Sentiment, Length: 17871, dtype: object"
            ],
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Sentiment</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>neutral</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>negative</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>positive</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>negative</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>positive</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>18403</th>\n",
              "      <td>positive</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>18404</th>\n",
              "      <td>positive</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>18405</th>\n",
              "      <td>neutral</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>18406</th>\n",
              "      <td>positive</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>18407</th>\n",
              "      <td>positive</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>17871 rows Ã— 1 columns</p>\n",
              "</div><br><label><b>dtype:</b> object</label>"
            ]
          },
          "metadata": {},
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# max_words = 7500\n",
        "\n",
        "dat = np.array(df)[:,0]\n",
        "str2id = dict()\n",
        "n_ids = 1\n",
        "max_str = 0\n",
        "for i in range(len(dat)):\n",
        "  # if(len(str2id.keys() > max_words)): break\n",
        "  strs = dat[i].split()\n",
        "  if len(strs) > max_str:\n",
        "    max_str = len(strs)\n",
        "  for s in strs:\n",
        "    if s not in str2id.keys():\n",
        "      str2id[s] = n_ids\n",
        "      n_ids += 1\n",
        "    # if(len(str2id.keys() > max_words)): break\n",
        "\n",
        "transformed_dat = np.zeros((np.shape(dat)[0], max_str), dtype = np.int32)\n",
        "for i in range(len(dat)):\n",
        "    strs = dat[i].split()\n",
        "    for s in range(100):\n",
        "      if(s >= len(strs)):\n",
        "        transformed_dat[i, s] = 0\n",
        "      else:\n",
        "        transformed_dat[i, s] = str2id[strs[s]]\n",
        "max_str = 100\n",
        "\n",
        "# print(transformed_y)\n",
        "\n",
        "vals, idxs, occs = np.unique(ys, return_inverse = True, return_counts = True)\n",
        "# print(vals, idxs, occs)\n",
        "\n",
        "transformed_dat, ys = load_dataset(transformed_dat, np.array(ys))\n",
        "transformed_y = np.zeros((np.shape(transformed_dat)[0]))\n",
        "transformed_y[ys == 'positive'] = 2\n",
        "transformed_y[ys == 'neutral'] =  1\n",
        "transformed_y[ys == 'negative'] = 0\n",
        "\n",
        "# transformed_y = np.zeros((np.shape(transformed_dat)[0], 3))\n",
        "# transformed_y[ys == 'positive'] = [1,0,0]\n",
        "# transformed_y[ys == 'neutral'] =  [0,1,0]\n",
        "# transformed_y[ys == 'negative'] = [0,0,1]\n",
        "\n",
        "# print(transformed_dat, transformed_y)\n",
        "\n"
      ],
      "metadata": {
        "id": "JssdUI2btkLR"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from torch import nn\n",
        "\n",
        "class myRNN(nn.Module):\n",
        "  def __init__(self, input_dim, embed_dim, hidden_dim, output_dim):\n",
        "    super(myRNN, self).__init__()\n",
        "    self.hidden_dim = hidden_dim\n",
        "    self.embed = nn.Embedding(input_dim, embed_dim, padding_idx=0)\n",
        "    self.drop = nn.Dropout(0.3)\n",
        "    self.rnn = nn.RNN(embed_dim, hidden_dim, nonlinearity='relu', batch_first = True)\n",
        "    self.final = nn.Linear(hidden_dim, output_dim)\n",
        "    self.output = nn.Softmax(dim=1)\n",
        "\n",
        "\n",
        "  def forward(self, x):\n",
        "    em = self.embed(x)\n",
        "    dropped = self.drop(em)\n",
        "    _, res = self.rnn(dropped)\n",
        "    dropped = self.drop(res)\n",
        "    final = self.final(dropped[-1])\n",
        "    return self.output(final)\n",
        "\n",
        "\n",
        "\n",
        "class myLSTM(nn.Module):\n",
        "  def __init__(self, input_dim, embed_dim, hidden_dim, output_dim):\n",
        "    super().__init__()\n",
        "    self.embed = nn.Embedding(input_dim, embed_dim)\n",
        "    self.lstm = nn.LSTM(embed_dim, hidden_dim, batch_first = True)\n",
        "    self.drop = nn.Dropout(0.3)\n",
        "    self.final = nn.Linear(hidden_dim, output_dim)\n",
        "    self.output = nn.Softmax(dim=1)\n",
        "\n",
        "\n",
        "  def forward(self, x):\n",
        "    em = self.embed(x)\n",
        "    dropped = self.drop(em)\n",
        "    _, (res, _) = self.lstm(dropped)\n",
        "    dropped = self.drop(res)\n",
        "    final = self.final(dropped[-1])\n",
        "    return self.output(final)\n",
        "\n",
        "\n",
        "class myGRU(nn.Module):\n",
        "\n",
        "  def __init__(self, input_dim, embed_dim, hidden_dim, output_dim):\n",
        "    super().__init__()\n",
        "    self.embed = nn.Embedding(input_dim, embed_dim)\n",
        "    self.gru = nn.GRU(embed_dim, hidden_dim, batch_first = True)\n",
        "    self.drop = nn.Dropout(0.3)\n",
        "    self.final = nn.Linear(hidden_dim, output_dim)\n",
        "    self.output = nn.Softmax(dim=1)\n",
        "\n",
        "\n",
        "  def forward(self, x):\n",
        "    em = self.embed(x)\n",
        "    dropped = self.drop(em)\n",
        "    _, res = self.gru(dropped)\n",
        "    dropped = self.drop(res)\n",
        "    final = self.final(dropped[-1])\n",
        "    return self.output(final)\n",
        "\n",
        ""
      ],
      "metadata": {
        "id": "y0hGVhzGGg3w"
      },
      "execution_count": 36,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from torch import optim\n",
        "from sklearn.metrics import f1_score, recall_score, precision_score, accuracy_score\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "def train(model, loss_fn, optimizer, X, y, X_val, y_val, X_test, epochs, val_epoch, name):\n",
        "  val_losses = []\n",
        "  train_losses = []\n",
        "  for e in range(epochs):\n",
        "    model.train()\n",
        "    e_loss = []\n",
        "    for batch in range(X.shape[0]//batch_size):\n",
        "      optimizer.zero_grad()\n",
        "      pred = model(X[batch*batch_size:(batch+1)*batch_size])\n",
        "      loss = loss_fn(pred, y[batch*batch_size:(batch+1)*batch_size])\n",
        "      e_loss.append(loss.item())\n",
        "      loss.backward()\n",
        "      optimizer.step()\n",
        "    train_losses.append(sum(e_loss)/len(e_loss))\n",
        "    print(\"Epoch {}: {:.5f}\".format(e, train_losses[-1]))\n",
        "\n",
        "    if e % val_epoch == 0:\n",
        "      model.eval()\n",
        "      with torch.no_grad():\n",
        "        pred = model(X_val)\n",
        "        loss = loss_fn(pred, y_val)\n",
        "        print(\"Validation loss: {:.5f}\".format(loss.item()))\n",
        "        print(\"Validation accuracy: {:.4f}\".format(accuracy_score(y_val, np.argmax(pred, axis=1))))\n",
        "        val_losses.append(loss.item())\n",
        "        # if val_losses[-1] <= 1.03:\n",
        "        #   break\n",
        "        if len(val_losses) > 50//val_epoch and np.mean(val_losses[-50//val_epoch:-25//val_epoch]) <= np.mean(val_losses[-25//val_epoch:]):\n",
        "          break\n",
        "    print()\n",
        "  torch.save({\n",
        "      'model_state_dict': model.state_dict(),\n",
        "      'optimizer_state_dict': optimizer.state_dict(),\n",
        "      'train_losses': train_losses,\n",
        "      'valid_losses': val_losses}, \"/content/drive/MyDrive/Colab Notebooks/{}.pt\".format(name))\n",
        "\n",
        "  test_pred = None\n",
        "  model.eval()\n",
        "  with torch.no_grad():\n",
        "    test_pred = model(X_test)\n",
        "  return train_losses, val_losses, test_pred\n"
      ],
      "metadata": {
        "id": "9JHCPbPawufG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from math import e\n",
        "from torch import optim\n",
        "from sklearn.metrics import f1_score, recall_score, precision_score, accuracy_score\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# indices = np.arange(np.shape(transformed_dat)[0])\n",
        "# np.random.shuffle(indices)\n",
        "# train_x = torch.tensor(transformed_dat[indices[:int(len(indices)*0.6)]], dtype = torch.long)\n",
        "# train_y = torch.tensor(transformed_y[indices[:int(len(indices)*0.6)]], dtype = torch.long)\n",
        "# val_x = torch.tensor(transformed_dat[indices[int(len(indices)*0.6):int(len(indices)*0.8)]], dtype = torch.long)\n",
        "# val_y = torch.tensor(transformed_y[indices[int(len(indices)*0.6):int(len(indices)*0.8)]], dtype = torch.long)\n",
        "# test_x = torch.tensor(transformed_dat[indices[int(len(indices)*0.8):]], dtype = torch.long)\n",
        "# test_y = torch.tensor(transformed_y[indices[int(len(indices)*0.8):]], dtype = torch.long)\n",
        "comments = df['Cleaned_Text']\n",
        "labels = df['Sentiment']\n",
        "labels[labels == 'positive'] = 2\n",
        "labels[labels == 'neutral'] = 1\n",
        "labels[labels == 'negative'] = 0\n",
        "comments = np.array(comments)\n",
        "labels = np.array(labels, dtype=np.uint8)\n",
        "\n",
        "tokenized_comments = [comment.split() for comment in comments]\n",
        "vocab = set([word for comment in tokenized_comments for word in comment])\n",
        "vocab = {word: idx+1 for idx, word in enumerate(vocab)}  # idx+1 to reserve 0 for padding\n",
        "vocab['<PAD>'] = 0\n",
        "\n",
        "# Padding\n",
        "def pad_sequences(tokenized_comments, max_len=100):\n",
        "    padded = []\n",
        "    for comment in tokenized_comments:\n",
        "        if len(comment) > max_len:\n",
        "            padded.append(comment[:max_len])\n",
        "        else:\n",
        "            padded.append(comment + ['<PAD>'] * (max_len - len(comment)))\n",
        "    return padded\n",
        "\n",
        "padded_comments = pad_sequences(tokenized_comments)\n",
        "\n",
        "# Encoding\n",
        "encoded_comments = [[vocab[word] for word in comment] for comment in padded_comments]\n",
        "\n",
        "# Train-Test Split\n",
        "X_train, X_test, y_train, y_test = train_test_split(encoded_comments, labels, test_size=0.2, random_state=42, stratify=labels)\n",
        "X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=0.25, random_state=42, stratify=y_train)\n",
        "X_train, y_train = load_dataset(np.array(X_train), np.array(y_train))\n",
        "X_val, y_val = load_dataset(np.array(X_val), np.array(y_val))\n",
        "X_test, y_test = load_dataset(np.array(X_test), np.array(y_test))\n",
        "X_train = torch.tensor(X_train, dtype = torch.long)\n",
        "X_test = torch.tensor(X_test, dtype = torch.long)\n",
        "X_val = torch.tensor(X_val, dtype = torch.long)\n",
        "y_val = torch.tensor(y_val, dtype = torch.long)\n",
        "y_train = torch.tensor(y_train, dtype = torch.long)\n",
        "y_test = torch.tensor(y_test, dtype = torch.long)\n",
        "\n",
        "epochs = 200\n",
        "batch_size = 32\n",
        "val_epoch = 5\n",
        "load_pretrained = False\n",
        "train_l_rnn, val_l_rnn, test_pred_rnn, train_l_lstm, val_l_lstm, test_pred_lstm, train_l_gru, val_l_gru, test_pred_gru = None, None, None, None, None, None, None, None, None\n",
        "\n",
        "if load_pretrained:\n",
        "  model = myRNN(len(vocab), 100, 128, 3)\n",
        "  optimizer = optim.AdamW(model.parameters(), lr = 0.001)\n",
        "  checkpoint = torch.load(\"/content/drive/MyDrive/Colab Notebooks/Simple_RNN.pt\")\n",
        "  model.load_state_dict(checkpoint['model_state_dict'])\n",
        "  optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
        "  train_l_rnn, val_l_rnn = checkpoint['train_losses'], checkpoint['valid_losses']\n",
        "  model.eval()\n",
        "  with torch.no_grad():\n",
        "    test_pred_rnn = model(X_test)\n",
        "\n",
        "  model = myLSTM(len(vocab), 100, 128, 3)\n",
        "  optimizer = optim.AdamW(model.parameters(), lr = 0.001)\n",
        "  checkpoint = torch.load(\"/content/drive/MyDrive/Colab Notebooks/LSTM.pt\")\n",
        "  model.load_state_dict(checkpoint['model_state_dict'])\n",
        "  optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
        "  train_l_lstm, val_l_lstm = checkpoint['train_losses'], checkpoint['valid_losses']\n",
        "  model.eval()\n",
        "  with torch.no_grad():\n",
        "    test_pred_lstm = model(X_test)\n",
        "\n",
        "  model = myGRU(len(vocab), 100, 128, 3)\n",
        "  optimizer = optim.AdamW(model.parameters(), lr = 0.001)\n",
        "  checkpoint = torch.load(\"/content/drive/MyDrive/Colab Notebooks/GRU.pt\")\n",
        "  model.load_state_dict(checkpoint['model_state_dict'])\n",
        "  optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
        "  train_l_gru, val_l_gru = checkpoint['train_losses'], checkpoint['valid_losses']\n",
        "  model.eval()\n",
        "  with torch.no_grad():\n",
        "    test_pred_gru = model(X_test)\n",
        "\n",
        "else:\n",
        "  model = myRNN(len(vocab), 100, 128, 3)\n",
        "  loss_fn = nn.CrossEntropyLoss()\n",
        "  optimizer = optim.AdamW(model.parameters(), lr = 0.001)\n",
        "  train_l_rnn, val_l_rnn, test_pred_rnn = train(model, loss_fn, optimizer, X_train, y_train, X_val, y_val, X_test, epochs, val_epoch, 'Simple_RNN')\n",
        "  print(\"-========= Done with simple RNN! ==========- \\n\")\n",
        "\n",
        "  model = myLSTM(len(vocab), 100, 128, 3)\n",
        "  loss_fn = nn.CrossEntropyLoss()\n",
        "  optimizer = optim.AdamW(model.parameters(), lr = 0.001)\n",
        "  train_l_lstm, val_l_lstm, test_pred_lstm = train(model, loss_fn, optimizer, X_train, y_train, X_val, y_val, X_test, epochs, val_epoch, 'LSTM')\n",
        "  print(\"-========= Done with LSTM! ==========- \\n\")\n",
        "\n",
        "  model = myGRU(len(vocab), 100, 128, 3)\n",
        "  loss_fn = nn.CrossEntropyLoss()\n",
        "  optimizer = optim.AdamW(model.parameters(), lr = 0.001)\n",
        "  train_l_gru, val_l_gru, test_pred_gru = train(model, loss_fn, optimizer, X_train, y_train, X_val, y_val, X_test, epochs, val_epoch, 'GRU')\n",
        "  print(\"-========= Done with GRU! ==========- \\n\")\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "pred = np.argmax(test_pred_rnn, axis=1)\n",
        "print(\"\\nSimple RNN Test metrics:\")\n",
        "print(\"f1 score: {:.5f}\".format(f1_score(y_test, pred, average='macro')))\n",
        "print(\"recall score: {:.5f}\".format(recall_score(y_test, pred, average='macro')))\n",
        "print(\"precision score: {:.5f}\".format(precision_score(y_test, pred, average='macro')))\n",
        "print(\"Overall accuracy: {:.2f}%\".format(accuracy_score(y_test, pred)*100))\n",
        "\n",
        "pred = np.argmax(test_pred_lstm, axis=1)\n",
        "print(\"\\nLSTM Test metrics:\")\n",
        "print(\"f1 score: {:.5f}\".format(f1_score(y_test, pred, average='macro')))\n",
        "print(\"recall score: {:.5f}\".format(recall_score(y_test, pred, average='macro')))\n",
        "print(\"precision score: {:.5f}\".format(precision_score(y_test, pred, average='macro')))\n",
        "print(\"Overall accuracy: {:.2f}%\".format(accuracy_score(y_test, pred)*100))\n",
        "\n",
        "pred = np.argmax(test_pred_gru, axis=1)\n",
        "print(\"\\nGRU Test metrics:\")\n",
        "print(\"f1 score: {:.5f}\".format(f1_score(y_test, pred, average='macro')))\n",
        "print(\"recall score: {:.5f}\".format(recall_score(y_test, pred, average='macro')))\n",
        "print(\"precision score: {:.5f}\".format(precision_score(y_test, pred, average='macro')))\n",
        "print(\"Overall accuracy: {:.2f}%\".format(accuracy_score(y_test, pred)*100))\n",
        "\n",
        "\n",
        "plt.plot(np.arange(len(train_l_rnn)), train_l_rnn, label=\"Training Loss\")\n",
        "plt.plot(np.arange(len(train_l_rnn), step=val_epoch), val_l_rnn, label=\"Validation Loss\")\n",
        "plt.legend()\n",
        "plt.xlabel(\"Epoch\")\n",
        "plt.ylabel(\"Loss\")\n",
        "plt.title(\"Loss vs. Epoch for Simple RNN\")\n",
        "plt.show()\n",
        "\n",
        "plt.plot(np.arange(len(train_l_lstm)), train_l_lstm, label=\"Training Loss\")\n",
        "plt.plot(np.arange(len(train_l_lstm), step=val_epoch), val_l_lstm, label=\"Validation Loss\")\n",
        "plt.legend()\n",
        "plt.xlabel(\"Epoch\")\n",
        "plt.ylabel(\"Loss\")\n",
        "plt.title(\"Loss vs. Epoch for LSTM\")\n",
        "plt.show()\n",
        "\n",
        "plt.plot(np.arange(len(train_l_gru)), train_l_gru, label=\"Training Loss\")\n",
        "plt.plot(np.arange(len(train_l_gru), step=val_epoch), val_l_gru, label=\"Validation Loss\")\n",
        "plt.legend()\n",
        "plt.xlabel(\"Epoch\")\n",
        "plt.ylabel(\"Loss\")\n",
        "plt.title(\"Loss vs. Epoch for GRU\")\n",
        "plt.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "l8ldG6P271e_",
        "outputId": "bd59488b-ba39-4c60-966f-01c7d57e4f05"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 0: 1.28609\n",
            "Validation loss: 1.21811\n",
            "Validation accuracy: 0.3333\n",
            "\n",
            "Epoch 1: 1.21731\n",
            "\n",
            "Epoch 2: 1.21731\n",
            "\n",
            "Epoch 3: 1.21731\n",
            "\n",
            "Epoch 4: 1.21731\n",
            "\n",
            "Epoch 5: 1.21731\n",
            "Validation loss: 1.21811\n",
            "Validation accuracy: 0.3333\n",
            "\n",
            "Epoch 6: 1.21731\n",
            "\n",
            "Epoch 7: 1.21731\n",
            "\n",
            "Epoch 8: 1.21731\n",
            "\n",
            "Epoch 9: 1.21731\n",
            "\n",
            "Epoch 10: 1.21731\n",
            "Validation loss: 1.21811\n",
            "Validation accuracy: 0.3333\n",
            "\n",
            "Epoch 11: 1.21731\n",
            "\n",
            "Epoch 12: 1.21731\n",
            "\n",
            "Epoch 13: 1.21731\n",
            "\n",
            "Epoch 14: 1.21731\n",
            "\n",
            "Epoch 15: 1.21731\n",
            "Validation loss: 1.21811\n",
            "Validation accuracy: 0.3333\n",
            "\n",
            "Epoch 16: 1.21731\n",
            "\n",
            "Epoch 17: 1.21731\n",
            "\n",
            "Epoch 18: 1.21731\n",
            "\n",
            "Epoch 19: 1.21731\n",
            "\n",
            "Epoch 20: 1.21731\n",
            "Validation loss: 1.21811\n",
            "Validation accuracy: 0.3333\n",
            "\n",
            "Epoch 21: 1.21731\n",
            "\n",
            "Epoch 22: 1.21731\n",
            "\n",
            "Epoch 23: 1.21731\n",
            "\n",
            "Epoch 24: 1.21731\n",
            "\n",
            "Epoch 25: 1.21731\n",
            "Validation loss: 1.21811\n",
            "Validation accuracy: 0.3333\n",
            "\n",
            "Epoch 26: 1.21731\n",
            "\n",
            "Epoch 27: 1.21731\n",
            "\n",
            "Epoch 28: 1.21731\n",
            "\n",
            "Epoch 29: 1.21731\n",
            "\n",
            "Epoch 30: 1.21731\n",
            "Validation loss: 1.21811\n",
            "Validation accuracy: 0.3333\n",
            "\n",
            "Epoch 31: 1.21731\n",
            "\n",
            "Epoch 32: 1.21731\n",
            "\n",
            "Epoch 33: 1.21731\n",
            "\n",
            "Epoch 34: 1.21731\n",
            "\n",
            "Epoch 35: 1.21731\n",
            "Validation loss: 1.21811\n",
            "Validation accuracy: 0.3333\n",
            "\n",
            "Epoch 36: 1.21731\n",
            "\n",
            "Epoch 37: 1.21731\n",
            "\n",
            "Epoch 38: 1.21731\n",
            "\n",
            "Epoch 39: 1.21731\n",
            "\n",
            "Epoch 40: 1.21731\n",
            "Validation loss: 1.21811\n",
            "Validation accuracy: 0.3333\n",
            "\n",
            "Epoch 41: 1.21731\n",
            "\n",
            "Epoch 42: 1.21731\n",
            "\n",
            "Epoch 43: 1.21731\n",
            "\n",
            "Epoch 44: 1.21731\n",
            "\n",
            "Epoch 45: 1.21731\n",
            "Validation loss: 1.21811\n",
            "Validation accuracy: 0.3333\n",
            "\n",
            "Epoch 46: 1.21731\n",
            "\n",
            "Epoch 47: 1.21731\n",
            "\n",
            "Epoch 48: 1.21731\n",
            "\n",
            "Epoch 49: 1.21731\n",
            "\n",
            "Epoch 50: 1.21731\n",
            "Validation loss: 1.21811\n",
            "Validation accuracy: 0.3333\n",
            "-========= Done with simple RNN! ==========- \n",
            "\n",
            "Epoch 0: 1.23861\n",
            "Validation loss: 1.21757\n",
            "Validation accuracy: 0.3333\n",
            "\n",
            "Epoch 1: 1.18821\n",
            "\n",
            "Epoch 2: 1.34029\n",
            "\n",
            "Epoch 3: 1.21633\n",
            "\n",
            "Epoch 4: 1.21073\n",
            "\n",
            "Epoch 5: 1.22510\n",
            "Validation loss: 1.21093\n",
            "Validation accuracy: 0.3333\n",
            "\n",
            "Epoch 6: 1.17011\n",
            "\n",
            "Epoch 7: 1.26517\n",
            "\n",
            "Epoch 8: 1.20918\n",
            "\n",
            "Epoch 9: 1.27761\n",
            "\n",
            "Epoch 10: 1.20037\n",
            "Validation loss: 1.11140\n",
            "Validation accuracy: 0.3355\n",
            "\n",
            "Epoch 11: 1.15009\n",
            "\n",
            "Epoch 12: 1.13706\n",
            "\n",
            "Epoch 13: 1.11505\n",
            "\n",
            "Epoch 14: 1.12323\n",
            "\n",
            "Epoch 15: 1.11623\n",
            "Validation loss: 1.09941\n",
            "Validation accuracy: 0.3484\n",
            "\n",
            "Epoch 16: 1.10830\n",
            "\n",
            "Epoch 17: 1.10778\n",
            "\n",
            "Epoch 18: 1.10577\n",
            "\n",
            "Epoch 19: 1.10352\n",
            "\n",
            "Epoch 20: 1.09988\n",
            "Validation loss: 1.10170\n",
            "Validation accuracy: 0.3420\n",
            "\n",
            "Epoch 21: 1.10069\n",
            "\n",
            "Epoch 22: 1.10063\n",
            "\n",
            "Epoch 23: 1.09812\n",
            "\n",
            "Epoch 24: 1.10040\n",
            "\n",
            "Epoch 25: 1.09802\n",
            "Validation loss: 1.10226\n",
            "Validation accuracy: 0.3398\n",
            "\n",
            "Epoch 26: 1.08191\n",
            "\n",
            "Epoch 27: 1.08458\n",
            "\n",
            "Epoch 28: 1.16384\n",
            "\n",
            "Epoch 29: 1.09907\n",
            "\n",
            "Epoch 30: 1.09508\n",
            "Validation loss: 1.10032\n",
            "Validation accuracy: 0.3441\n",
            "\n",
            "Epoch 31: 1.09227\n",
            "\n",
            "Epoch 32: 1.08868\n",
            "\n",
            "Epoch 33: 1.10263\n",
            "\n",
            "Epoch 34: 1.07165\n",
            "\n",
            "Epoch 35: 1.08561\n",
            "Validation loss: 1.10626\n",
            "Validation accuracy: 0.3420\n",
            "\n",
            "Epoch 36: 1.09551\n",
            "\n",
            "Epoch 37: 1.08020\n",
            "\n",
            "Epoch 38: 1.05501\n",
            "\n",
            "Epoch 39: 1.02897\n",
            "\n",
            "Epoch 40: 1.04168\n",
            "Validation loss: 1.08795\n",
            "Validation accuracy: 0.3865\n",
            "\n",
            "Epoch 41: 0.98940\n",
            "\n",
            "Epoch 42: 0.96916\n",
            "\n",
            "Epoch 43: 0.94390\n",
            "\n",
            "Epoch 44: 0.93269\n",
            "\n",
            "Epoch 45: 0.91877\n",
            "Validation loss: 1.08828\n",
            "Validation accuracy: 0.4023\n",
            "\n",
            "Epoch 46: 0.89456\n",
            "\n",
            "Epoch 47: 0.87143\n",
            "\n",
            "Epoch 48: 0.85551\n",
            "\n",
            "Epoch 49: 0.83640\n",
            "\n",
            "Epoch 50: 0.81726\n",
            "Validation loss: 1.08355\n",
            "Validation accuracy: 0.4397\n",
            "\n",
            "Epoch 51: 0.79405\n",
            "\n",
            "Epoch 52: 0.79227\n",
            "\n",
            "Epoch 53: 0.77216\n",
            "\n",
            "Epoch 54: 0.76874\n",
            "\n",
            "Epoch 55: 0.75302\n",
            "Validation loss: 1.08481\n",
            "Validation accuracy: 0.4447\n",
            "\n",
            "Epoch 56: 0.74843\n",
            "\n",
            "Epoch 57: 0.73385\n",
            "\n",
            "Epoch 58: 0.73046\n",
            "\n",
            "Epoch 59: 0.73294\n",
            "\n",
            "Epoch 60: 0.71649\n",
            "Validation loss: 1.07998\n",
            "Validation accuracy: 0.4476\n",
            "\n",
            "Epoch 61: 0.70454\n",
            "\n",
            "Epoch 62: 0.71005\n",
            "\n",
            "Epoch 63: 0.70773\n",
            "\n",
            "Epoch 64: 0.69945\n",
            "\n",
            "Epoch 65: 0.68468\n",
            "Validation loss: 1.08498\n",
            "Validation accuracy: 0.4540\n",
            "\n",
            "Epoch 66: 0.68020\n",
            "\n",
            "Epoch 67: 0.67135\n",
            "\n",
            "Epoch 68: 0.66942\n",
            "\n",
            "Epoch 69: 0.66471\n",
            "\n",
            "Epoch 70: 0.65968\n",
            "Validation loss: 1.08238\n",
            "Validation accuracy: 0.4591\n",
            "\n",
            "Epoch 71: 0.65203\n",
            "\n",
            "Epoch 72: 0.65025\n",
            "\n",
            "Epoch 73: 0.65351\n",
            "\n",
            "Epoch 74: 0.64877\n",
            "\n",
            "Epoch 75: 0.64444\n",
            "Validation loss: 1.08085\n",
            "Validation accuracy: 0.4626\n",
            "\n",
            "Epoch 76: 0.64697\n",
            "\n",
            "Epoch 77: 0.63830\n",
            "\n",
            "Epoch 78: 0.63725\n",
            "\n",
            "Epoch 79: 0.63324\n",
            "\n",
            "Epoch 80: 0.62839\n",
            "Validation loss: 1.06437\n",
            "Validation accuracy: 0.4777\n",
            "\n",
            "Epoch 81: 0.62661\n",
            "\n",
            "Epoch 82: 0.62801\n",
            "\n",
            "Epoch 83: 0.62243\n",
            "\n",
            "Epoch 84: 0.61984\n",
            "\n",
            "Epoch 85: 0.61326\n",
            "Validation loss: 1.06084\n",
            "Validation accuracy: 0.4799\n",
            "\n",
            "Epoch 86: 0.61774\n",
            "\n",
            "Epoch 87: 0.61684\n",
            "\n",
            "Epoch 88: 0.62140\n",
            "\n",
            "Epoch 89: 0.61470\n",
            "\n",
            "Epoch 90: 0.61786\n",
            "Validation loss: 1.08184\n",
            "Validation accuracy: 0.4619\n",
            "\n",
            "Epoch 91: 0.61642\n",
            "\n",
            "Epoch 92: 0.62499\n",
            "\n",
            "Epoch 93: 0.62165\n",
            "\n",
            "Epoch 94: 0.61306\n",
            "\n",
            "Epoch 95: 0.60635\n",
            "Validation loss: 1.07189\n",
            "Validation accuracy: 0.4741\n",
            "\n",
            "Epoch 96: 0.60844\n",
            "\n",
            "Epoch 97: 0.61036\n",
            "\n",
            "Epoch 98: 0.60695\n",
            "\n",
            "Epoch 99: 0.60474\n",
            "\n",
            "Epoch 100: 0.60626\n",
            "Validation loss: 1.06107\n",
            "Validation accuracy: 0.4849\n",
            "\n",
            "Epoch 101: 0.60272\n",
            "\n",
            "Epoch 102: 0.60508\n",
            "\n",
            "Epoch 103: 0.60155\n",
            "\n",
            "Epoch 104: 0.59951\n",
            "\n",
            "Epoch 105: 0.60008\n",
            "Validation loss: 1.06442\n",
            "Validation accuracy: 0.4813\n",
            "\n",
            "Epoch 106: 0.60370\n",
            "\n",
            "Epoch 107: 0.59820\n",
            "\n",
            "Epoch 108: 0.59870\n",
            "\n",
            "Epoch 109: 0.59569\n",
            "\n",
            "Epoch 110: 0.59734\n",
            "Validation loss: 1.07725\n",
            "Validation accuracy: 0.4691\n",
            "\n",
            "Epoch 111: 0.60165\n",
            "\n",
            "Epoch 112: 0.60227\n",
            "\n",
            "Epoch 113: 0.59814\n",
            "\n",
            "Epoch 114: 0.59320\n",
            "\n",
            "Epoch 115: 0.60368\n",
            "Validation loss: 1.06537\n",
            "Validation accuracy: 0.4799\n",
            "\n",
            "Epoch 116: 0.59324\n",
            "\n",
            "Epoch 117: 0.59267\n",
            "\n",
            "Epoch 118: 0.59316\n",
            "\n",
            "Epoch 119: 0.59238\n",
            "\n",
            "Epoch 120: 0.60043\n",
            "Validation loss: 1.06072\n",
            "Validation accuracy: 0.4885\n",
            "\n",
            "Epoch 121: 0.59347\n",
            "\n",
            "Epoch 122: 0.58969\n",
            "\n",
            "Epoch 123: 0.59094\n",
            "\n",
            "Epoch 124: 0.59917\n",
            "\n",
            "Epoch 125: 0.59338\n",
            "Validation loss: 1.06040\n",
            "Validation accuracy: 0.4856\n",
            "\n",
            "Epoch 126: 0.59580\n",
            "\n",
            "Epoch 127: 0.59245\n",
            "\n",
            "Epoch 128: 0.59722\n",
            "\n",
            "Epoch 129: 0.59643\n",
            "\n",
            "Epoch 130: 0.59166\n",
            "Validation loss: 1.07054\n",
            "Validation accuracy: 0.4749\n",
            "\n",
            "Epoch 131: 0.59162\n",
            "\n",
            "Epoch 132: 0.59004\n",
            "\n",
            "Epoch 133: 0.59077\n",
            "\n",
            "Epoch 134: 0.58833\n",
            "\n",
            "Epoch 135: 0.59189\n",
            "Validation loss: 1.05956\n",
            "Validation accuracy: 0.4899\n",
            "\n",
            "Epoch 136: 0.58567\n",
            "\n",
            "Epoch 137: 0.59428\n",
            "\n",
            "Epoch 138: 0.59093\n",
            "\n",
            "Epoch 139: 0.58939\n",
            "\n",
            "Epoch 140: 0.58700\n",
            "Validation loss: 1.05920\n",
            "Validation accuracy: 0.4885\n",
            "\n",
            "Epoch 141: 0.59105\n",
            "\n",
            "Epoch 142: 0.58816\n",
            "\n",
            "Epoch 143: 0.58837\n",
            "\n",
            "Epoch 144: 0.58923\n",
            "\n",
            "Epoch 145: 0.59391\n",
            "Validation loss: 1.06992\n",
            "Validation accuracy: 0.4720\n",
            "\n",
            "Epoch 146: 0.59296\n",
            "\n",
            "Epoch 147: 0.58310\n",
            "\n",
            "Epoch 148: 0.58547\n",
            "\n",
            "Epoch 149: 0.58672\n",
            "\n",
            "Epoch 150: 0.57935\n",
            "Validation loss: 1.06084\n",
            "Validation accuracy: 0.4820\n",
            "\n",
            "Epoch 151: 0.58353\n",
            "\n",
            "Epoch 152: 0.58515\n",
            "\n",
            "Epoch 153: 0.58494\n",
            "\n",
            "Epoch 154: 0.58309\n",
            "\n",
            "Epoch 155: 0.58418\n",
            "Validation loss: 1.05507\n",
            "Validation accuracy: 0.4928\n",
            "\n",
            "Epoch 156: 0.58579\n",
            "\n",
            "Epoch 157: 0.58397\n",
            "\n",
            "Epoch 158: 0.58234\n",
            "\n",
            "Epoch 159: 0.58743\n",
            "\n",
            "Epoch 160: 0.58276\n",
            "Validation loss: 1.04534\n",
            "Validation accuracy: 0.5014\n",
            "\n",
            "Epoch 161: 0.58403\n",
            "\n",
            "Epoch 162: 0.58456\n",
            "\n",
            "Epoch 163: 0.58558\n",
            "\n",
            "Epoch 164: 0.58408\n",
            "\n",
            "Epoch 165: 0.58363\n",
            "Validation loss: 1.04412\n",
            "Validation accuracy: 0.5000\n",
            "\n",
            "Epoch 166: 0.58977\n",
            "\n",
            "Epoch 167: 0.58356\n",
            "\n",
            "Epoch 168: 0.58480\n",
            "\n",
            "Epoch 169: 0.58228\n",
            "\n",
            "Epoch 170: 0.58162\n",
            "Validation loss: 1.04150\n",
            "Validation accuracy: 0.5022\n",
            "\n",
            "Epoch 171: 0.57976\n",
            "\n",
            "Epoch 172: 0.58385\n",
            "\n",
            "Epoch 173: 0.57650\n",
            "\n",
            "Epoch 174: 0.57810\n",
            "\n",
            "Epoch 175: 0.57788\n",
            "Validation loss: 1.04463\n",
            "Validation accuracy: 0.5022\n",
            "\n",
            "Epoch 176: 0.58486\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!wget http://nlp.stanford.edu/data/glove.6B.zip\n",
        "!unzip -o glove.6B.zip"
      ],
      "metadata": {
        "id": "GWrNA8nnHfr2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install tensorflow"
      ],
      "metadata": {
        "id": "iEWBtLRRI2D-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "\n",
        "label_encoder = LabelEncoder()\n",
        "df[\"Sentiment\"] = label_encoder.fit_transform(df[\"Sentiment\"])\n",
        "\n",
        "# Tokenizer parameters for vocabulary size and maximum sequence length\n",
        "max_words = 7500\n",
        "max_length = 60\n",
        "\n",
        "X = df[\"Cleaned_Text\"]\n",
        "y = df[\"Sentiment\"]\n",
        "\n",
        "# Initialize tokenizer and learn word index mapping from dataset\n",
        "tokenizer = Tokenizer(num_words=max_words)\n",
        "tokenizer.fit_on_texts(X)\n",
        "\n",
        "# Convert text into numerical sequences\n",
        "X_sequences = tokenizer.texts_to_sequences(X)\n",
        "\n",
        "# Apply padding to standardize input lengths\n",
        "X_padded = pad_sequences(X_sequences, maxlen=max_length, padding=\"post\", truncating=\"post\")\n",
        "\n",
        "print(X_padded)\n",
        "\n",
        "# Load pretrained GloVe embeddings for improve word representation\n",
        "embedding_index = {}\n",
        "with open(\"glove.6B.300d.txt\", encoding=\"utf-8\") as f:\n",
        "    for line in f:\n",
        "        values = line.split()\n",
        "        word = values[0] # Extract the word\n",
        "        coefs = np.asarray(values[1:], dtype=\"float32\") # Convert embedding values to float\n",
        "        embedding_index[word] = coefs # Store word and corresponding vector\n",
        "\n",
        "# Create embedding matrix for the model\n",
        "embedding_matrix = np.zeros((min(len(tokenizer.word_index) + 1, max_words), 300))\n",
        "for word, i in tokenizer.word_index.items():\n",
        "    if i < max_words: # Only include words within the specified vocabulary size\n",
        "        embedding_vector = embedding_index.get(word) # Retrieve the pre-trained GloVe vector for the word\n",
        "        if embedding_vector is not None: # If word exists in GloVe embeddings\n",
        "            embedding_matrix[i] = embedding_vector # Assign the embedding vector to the matrix"
      ],
      "metadata": {
        "id": "QoG_3cq5IERD"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}