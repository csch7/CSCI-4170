{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyO0pYS5dQdLD2OT3X/tC5/3",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/csch7/CSCI-4170/blob/main/Homework-03/Neural_Networks.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Neural Networks\n",
        "### Author: Colin Scherer\n",
        "\n",
        "For this assignment, I've chosen a dataset of diabetes patients -- I'm trying to determine whether each patient will be readmitted to the hospital, and how soon. Although that sounds like a mix of classification and regression, this is in reality a multi-class classification problem with 3 classes: readmitted within 30 days, readmitted after 30 days, and not readmitted. As this dataset is very messy, I will have to do extensive preprocessing: any feature with more than 2% of values missing I will just remove -- this isn't too bad, since this only impacts features which intuitively wouldn't have much of an impact on the problem. I additionally convert all strings to an integer I thought made sense. Performing one-hot encoding on the categorical features would also be advised, but I ran out of time to do so. The targets of this dataset are also very imbalanced, with only 10% of the dataset being classified as one of 3 targets, and 60% of the dataset being classified as another. To solve this issue, I will downsample the larger portions randomly. I can afford to do this since the raw dataset has about 100k samples.\n",
        "\n",
        "Even with these problems addressed, the dataset in general is messy, with plenty of missing data and unbalanced variables. For example, I am generalizing \"NO\" for \"readmitted\" to mean the patient was never readmitted. In reality, a \"NO\" means there was no record of readmission -- many of these could just be records that were lost. Due to the bad quality of data, I'm not expecting to get high accuracies -- any accuracies greater than ~40% will be considered a win."
      ],
      "metadata": {
        "id": "hS39QL0UvLyx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd\n",
        "import re\n",
        "from google.colab import drive\n",
        "\n",
        "def repl_age(age: str) -> int:\n",
        "  return int(re.findall(r'\\d+', age)[0]) # Converts age string into an integer by taking the lower bound of the range.\n",
        "\n",
        "drive.mount('/content/drive')\n",
        "df = pd.read_csv(\"/content/drive/MyDrive/Colab Notebooks/diabetic_data.csv\")\n",
        "\n",
        "for c in df.columns:\n",
        "  pctUnknown = np.sum([df[c] == '?'])/df[c].shape[0]\n",
        "  if(pctUnknown > 0.02):\n",
        "    df.drop(c, axis = 1, inplace = True)\n",
        "  else:\n",
        "    df = df[df[c] != '?']\n",
        "\n",
        "# Transform strings into integers\n",
        "df.drop('diag_1', axis = 1, inplace = True)\n",
        "df.drop('diag_2', axis = 1, inplace = True)\n",
        "df.drop('diag_3', axis = 1, inplace = True)\n",
        "df.replace('Male', 0, inplace = True)\n",
        "df.replace('Female', 1, inplace = True)\n",
        "df = df[df['gender'] != 'Unknown/Invalid']\n",
        "df.replace('No', 0, inplace = True)\n",
        "df.replace('NO', 2, inplace = True)\n",
        "df.replace('>30', 1, inplace = True)\n",
        "df.replace('<30', 0, inplace = True)\n",
        "df.replace('Down', 1, inplace = True)\n",
        "df.replace('Steady', 2, inplace = True)\n",
        "df.replace('Up', 3, inplace = True)\n",
        "df.replace('Yes', 1, inplace = True)\n",
        "df.replace('Ch', 1, inplace = True)\n",
        "df.replace('Norm', 1, inplace = True)\n",
        "df.replace('>7', 2, inplace = True)\n",
        "df.replace('>8', 3, inplace = True)\n",
        "df.replace('>200', 2, inplace = True)\n",
        "df.replace('>300', 3, inplace = True)\n",
        "df.fillna(0, inplace = True)\n",
        "df['age'] = df['age'].apply(repl_age)\n",
        "\n",
        "tars = df['readmitted']\n",
        "df.drop('readmitted', axis = 1, inplace = True)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "P7re2mfwpgZN",
        "outputId": "affbd29a-a488-4e73-b9b0-4658ff743fac"
      },
      "execution_count": 316,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-316-d281ce0916d6>:27: FutureWarning: Downcasting behavior in `replace` is deprecated and will be removed in a future version. To retain the old behavior, explicitly call `result.infer_objects(copy=False)`. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n",
            "  df.replace('No', 0, inplace = True)\n",
            "<ipython-input-316-d281ce0916d6>:30: FutureWarning: Downcasting behavior in `replace` is deprecated and will be removed in a future version. To retain the old behavior, explicitly call `result.infer_objects(copy=False)`. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n",
            "  df.replace('<30', 0, inplace = True)\n",
            "<ipython-input-316-d281ce0916d6>:32: FutureWarning: Downcasting behavior in `replace` is deprecated and will be removed in a future version. To retain the old behavior, explicitly call `result.infer_objects(copy=False)`. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n",
            "  df.replace('Steady', 2, inplace = True)\n",
            "<ipython-input-316-d281ce0916d6>:33: FutureWarning: Downcasting behavior in `replace` is deprecated and will be removed in a future version. To retain the old behavior, explicitly call `result.infer_objects(copy=False)`. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n",
            "  df.replace('Up', 3, inplace = True)\n",
            "<ipython-input-316-d281ce0916d6>:34: FutureWarning: Downcasting behavior in `replace` is deprecated and will be removed in a future version. To retain the old behavior, explicitly call `result.infer_objects(copy=False)`. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n",
            "  df.replace('Yes', 1, inplace = True)\n",
            "<ipython-input-316-d281ce0916d6>:35: FutureWarning: Downcasting behavior in `replace` is deprecated and will be removed in a future version. To retain the old behavior, explicitly call `result.infer_objects(copy=False)`. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n",
            "  df.replace('Ch', 1, inplace = True)\n",
            "<ipython-input-316-d281ce0916d6>:38: FutureWarning: Downcasting behavior in `replace` is deprecated and will be removed in a future version. To retain the old behavior, explicitly call `result.infer_objects(copy=False)`. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n",
            "  df.replace('>8', 3, inplace = True)\n",
            "<ipython-input-316-d281ce0916d6>:40: FutureWarning: Downcasting behavior in `replace` is deprecated and will be removed in a future version. To retain the old behavior, explicitly call `result.infer_objects(copy=False)`. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n",
            "  df.replace('>300', 3, inplace = True)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from statsmodels.stats.outliers_influence import variance_inflation_factor\n",
        "\n",
        "dropped_features = []\n",
        "\n",
        "for i, feature in enumerate(df.columns):\n",
        "  vif = variance_inflation_factor(df, i)\n",
        "  print(\"{f}:\\t\\t\\t{v}\".format(f=feature, v=vif))\n",
        "  if vif > 5 or np.isnan(vif): # Remove any features with vif > 5.\n",
        "    dropped_features.append(feature)\n",
        "\n",
        "df.drop(dropped_features, axis = 1, inplace = True)\n",
        "df.drop('patient_nbr', axis = 1, inplace = True)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DCG4toEO4IoF",
        "outputId": "f6ad6754-f18c-4232-e3e3-3d007d5d1064"
      },
      "execution_count": 318,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "patient_nbr:\t\t\t2.6187727219405685\n",
            "gender:\t\t\t1.9767106176361124\n",
            "admission_type_id:\t\t\t3.0537541754831503\n",
            "discharge_disposition_id:\t\t\t1.5493688982755114\n",
            "admission_source_id:\t\t\t2.9887910431030305\n",
            "time_in_hospital:\t\t\t3.1912202094514175\n",
            "num_procedures:\t\t\t1.7395012916504649\n",
            "number_outpatient:\t\t\t1.1180938194445622\n",
            "number_emergency:\t\t\t1.1414915515245831\n",
            "number_inpatient:\t\t\t1.3781828685044712\n",
            "max_glu_serum:\t\t\t1.3918876293956812\n",
            "A1Cresult:\t\t\t1.1946062676438023\n",
            "metformin:\t\t\t1.4459896309297542\n",
            "repaglinide:\t\t\t1.0325326059033992\n",
            "nateglinide:\t\t\t1.0142990577027875\n",
            "chlorpropamide:\t\t\t1.0018860873801498\n",
            "glimepiride:\t\t\t1.110445304801677\n",
            "acetohexamide:\t\t\t1.0002580291888405\n",
            "glipizide:\t\t\t1.2615416515890447\n",
            "glyburide:\t\t\t1.242272289756267\n",
            "tolbutamide:\t\t\t1.0005351101108333\n",
            "pioglitazone:\t\t\t1.1547646225920742\n",
            "rosiglitazone:\t\t\t1.135545478683643\n",
            "acarbose:\t\t\t1.0073255839623099\n",
            "miglitol:\t\t\t1.0014137360291113\n",
            "troglitazone:\t\t\t1.0003599762364608\n",
            "tolazamide:\t\t\t1.0010978953284206\n",
            "insulin:\t\t\t2.687638722216703\n",
            "glyburide-metformin:\t\t\t1.0162265106198174\n",
            "glipizide-metformin:\t\t\t1.001283031730118\n",
            "glimepiride-pioglitazone:\t\t\t1.0000869927932896\n",
            "metformin-rosiglitazone:\t\t\t1.0002543804383133\n",
            "metformin-pioglitazone:\t\t\t1.0001962322656688\n",
            "change:\t\t\t3.59063075625552\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def load_dataset(dat, tar):\n",
        "  vals, cts = np.unique(tar, return_counts=True)\n",
        "  min_val = vals[np.argmin(cts)]\n",
        "  min_ct = min(cts)\n",
        "  new_dat = dat[tar == min_val]\n",
        "  new_tar = tar[tar == min_val]\n",
        "  for i, v in enumerate(vals):\n",
        "    if v != min_val:\n",
        "      indices = list(np.where(tar == v))[0]\n",
        "      new_ind = np.random.choice(indices, min_ct)\n",
        "      new_dat = np.append(new_dat, dat[new_ind], axis=0)\n",
        "      new_tar = np.append(new_tar, tar[new_ind])\n",
        "  return new_dat, new_tar\n",
        "\n",
        "\n",
        "def ReLu(z):\n",
        "  return np.maximum(z, 0)\n",
        "\n",
        "\n",
        "\n",
        "class myNeuralNetwork:\n",
        "  def __init__(self, train_data, train_targets, epochs, num_hidden_layers=2, hidden_dim=16, lr=0.01, batch_size = 32):\n",
        "    self.batch_size = batch_size\n",
        "    self.train_data = train_data\n",
        "    self.train_targets = train_targets\n",
        "    self.epochs = epochs\n",
        "    self.learning_rate = lr\n",
        "    self.hidden_layers = num_hidden_layers\n",
        "    self.weights = [np.random.uniform(-0.1, 0.1, (np.shape(train_data)[1], hidden_dim))]\n",
        "    self.biases = [np.random.uniform(-0.1, 0.1, (hidden_dim, 1))]\n",
        "    for i in range(1, num_hidden_layers):\n",
        "      self.weights.append(np.random.uniform(-0.1, 0.1, (hidden_dim, hidden_dim)))\n",
        "      self.biases.append(np.random.uniform(-0.1, 0.1, (hidden_dim, 1)))\n",
        "    self.weights.append(np.random.uniform(-0.1, 0.1, (hidden_dim, np.shape(train_targets)[1])))\n",
        "    self.biases.append(np.random.uniform(-0.1, 0.1, (np.shape(train_targets)[1], 1)))\n",
        "\n",
        "  def forward(self, data):\n",
        "    z = []\n",
        "    a = [data.T]\n",
        "    for l in range(self.hidden_layers+1):\n",
        "      z.append(np.matmul(self.weights[l].T, a[l]) + self.biases[l])\n",
        "      # print(a[-1])\n",
        "      if(l != self.hidden_layers):\n",
        "        a.append(ReLu(z[l]))\n",
        "      else:\n",
        "        a.append(np.exp(z[l])/np.sum(np.exp(z[l]), axis = 0))\n",
        "    return a, z\n",
        "\n",
        "  def backward(self, a, z, tars):\n",
        "    deltas = [0] * (self.hidden_layers+1)\n",
        "    # Calculate gradient for the last layer. The derivative of softmax is given by p_i - y_i\n",
        "    deltas[-1] = (a[-1]-tars.T).T\n",
        "    for l in range(self.hidden_layers-1, -1, -1):\n",
        "      deltas[l] = np.matmul(deltas[l+1], self.weights[l+1].T)*(np.where(z[l] > 0, 1, 0)).T\n",
        "    return deltas\n",
        "\n",
        "\n",
        "  def train(self):\n",
        "    N = np.shape(self.train_data)[0]\n",
        "    for e in range(self.epochs):\n",
        "      print(\"Epoch {}\".format(e))\n",
        "      for batch in range(np.shape(self.train_data)[0]//self.batch_size):\n",
        "        a, z = self.forward(self.train_data[self.batch_size*batch:self.batch_size*(batch+1)])\n",
        "        dels = self.backward(a, z, self.train_targets[self.batch_size*batch:self.batch_size*(batch+1)])\n",
        "\n",
        "        for l in range(self.hidden_layers+1):\n",
        "          self.weights[l] -= self.learning_rate*(np.matmul(dels[l].T, a[l].T)).T\n",
        "          self.biases[l] = self.biases[l]-self.learning_rate*np.array([np.mean(dels[l].T, axis=1)]).T\n",
        "        if(batch % 100 == 0):\n",
        "          print(\"Loss: {}\".format(self.cost(a[-1], self.train_targets[self.batch_size*batch:self.batch_size*(batch+1)])))\n",
        "\n",
        "\n",
        "  def cost(self, preds, tars):\n",
        "    return np.sum(-np.log(preds.T[range(np.shape(preds)[1]), np.argmax(tars, axis=1)]))/np.shape(preds)[1]\n",
        "\n",
        "  def predict(self, data):\n",
        "    probs, _ = self.forward(data)\n",
        "    probs = probs[-1]\n",
        "    return np.argmax(probs.T, axis=1)\n"
      ],
      "metadata": {
        "id": "B_J4_eDLqgt3"
      },
      "execution_count": 322,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "For implementing a 2-layer neural network with pytorch, I closely followed [this tutorial](https://pytorch.org/tutorials/beginner/basics/buildmodel_tutorial.html)."
      ],
      "metadata": {
        "id": "6Os0bZxnsmBA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from torch import nn\n",
        "from torch.utils.data import DataLoader\n",
        "from torchvision import datasets, transforms\n",
        "\n",
        "class torchNeuralNetwork(nn.Module):\n",
        "  def __init__(self, num_features, num_classes, num_hidden_layers=2, hidden_dim=16):\n",
        "    super().__init__()\n",
        "    self.layer_list = [nn.BatchNorm1d(num_features), nn.Linear(num_features, hidden_dim), nn.Tanh()]\n",
        "    for l in range(1, num_hidden_layers):\n",
        "      self.layer_list.append(nn.Linear(hidden_dim, hidden_dim))\n",
        "      self.layer_list.append(nn.Tanh())\n",
        "    self.layer_list.append(nn.Linear(hidden_dim, num_classes))\n",
        "    self.layer_list.append(nn.Softmax())\n",
        "    self.layers = nn.Sequential(*self.layer_list)\n",
        "\n",
        "  def forward(self, x):\n",
        "    return self.layers(x)\n"
      ],
      "metadata": {
        "id": "-L8QNgkBI1uT"
      },
      "execution_count": 274,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "For my training-validation-test split, I've chosen to take half of my dataset for training and sample validation and test sets at a 3:1:1 train:valid:test ratio from the rest of the dataset."
      ],
      "metadata": {
        "id": "YgEedrVVz3y5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "dat = np.array(df)\n",
        "tar = np.array(tars)\n",
        "dat, tar = load_dataset(dat, tar)\n",
        "print(np.shape(dat))\n",
        "N = np.shape(dat)[0]\n",
        "\n",
        "pct_train = 0.5\n",
        "pct_val = pct_train + pct_train*0.3\n",
        "pct_test = pct_val + pct_train*0.3\n",
        "indices = np.arange(N)\n",
        "np.random.shuffle(indices)\n",
        "\n",
        "targets = np.zeros((N, 3))\n",
        "targets[range(N), tar] = 1\n",
        "\n",
        "train_data = dat[indices[:int(pct_train*N)]]\n",
        "train_targets = targets[indices[:int(pct_train*N)]]\n",
        "val_data = dat[indices[int(pct_train*N):int(pct_val*N)]]\n",
        "val_targets = targets[[indices[int(pct_train*N):int(pct_val*N)]]]\n",
        "test_data = dat[indices[int(pct_val*N):int(pct_test*N)]]\n",
        "test_targets = targets[indices[int(pct_val*N):int(pct_test*N)]]\n",
        "mynn = myNeuralNetwork(train_data, train_targets, 100, lr=0.01)\n",
        "mynn.train(val_data, val_targets)\n",
        "preds = mynn.predict(test_data)\n",
        "print(preds)\n",
        "print(np.sum(test_targets[range(np.shape(test_targets)[0]), preds])/np.shape(test_targets)[0])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ko7CS6nL6qF3",
        "outputId": "659d05d0-6f2a-4abe-8a88-f206eb59fdd4"
      },
      "execution_count": 324,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(33750, 33)\n",
            "Epoch 0\n",
            "Loss: 1.116949316319236\n",
            "Loss: 1.0815245152896282\n",
            "Loss: 1.0471970289361037\n",
            "Loss: 1.067686024670261\n",
            "Loss: 1.021989519254288\n",
            "Loss: 1.1017323582360166\n",
            "Epoch 1\n",
            "Loss: 1.0089989714840717\n",
            "Loss: 1.0688308278227563\n",
            "Loss: 1.0502225181227394\n",
            "Loss: 1.0661093898824543\n",
            "Loss: 1.0324767350238457\n",
            "Loss: 1.0898361607061342\n",
            "Epoch 2\n",
            "Loss: 1.0233656327299656\n",
            "Loss: 1.0645191776642977\n",
            "Loss: 1.0524244848605497\n",
            "Loss: 1.06291999551147\n",
            "Loss: 1.0111712000256246\n",
            "Loss: 1.087391981427789\n",
            "Epoch 3\n",
            "Loss: 1.0168849899327186\n",
            "Loss: 1.072658288285803\n",
            "Loss: 1.0449283833739034\n",
            "Loss: 1.0541194812162429\n",
            "Loss: 1.0120800555459288\n",
            "Loss: 1.0887429270374627\n",
            "Epoch 4\n",
            "Loss: 1.0183565622613742\n",
            "Loss: 1.0525879598550065\n",
            "Loss: 1.0564841484096652\n",
            "Loss: 1.0503410521012064\n",
            "Loss: 0.9923703316795967\n",
            "Loss: 1.0755840225507445\n",
            "Epoch 5\n",
            "Loss: 1.0201594607570752\n",
            "Loss: 1.0448586745844024\n",
            "Loss: 1.0612754018875208\n",
            "Loss: 1.0510821982505623\n",
            "Loss: 1.0013734757732768\n",
            "Loss: 1.0767010674832402\n",
            "Epoch 6\n",
            "Loss: 1.0213557644866809\n",
            "Loss: 1.045637322974415\n",
            "Loss: 1.0787416424786656\n",
            "Loss: 1.0455955778733865\n",
            "Loss: 0.9762450160053195\n",
            "Loss: 1.08057047582952\n",
            "Epoch 7\n",
            "Loss: 1.0326395624097537\n",
            "Loss: 1.0364179521024195\n",
            "Loss: 1.0616807328410705\n",
            "Loss: 1.044464287335661\n",
            "Loss: 0.9900299470526905\n",
            "Loss: 1.072666809918359\n",
            "Epoch 8\n",
            "Loss: 1.0040418416405368\n",
            "Loss: 1.0375810976628614\n",
            "Loss: 1.1131456256897898\n",
            "Loss: 1.0837224253702873\n",
            "Loss: 0.9914697828039446\n",
            "Loss: 1.0676940373542185\n",
            "Epoch 9\n",
            "Loss: 1.0347446992094613\n",
            "Loss: 1.0338147438327565\n",
            "Loss: 1.0978403154314729\n",
            "Loss: 1.069611010237704\n",
            "Loss: 0.9975562623644362\n",
            "Loss: 1.0756914571339777\n",
            "Epoch 10\n",
            "Loss: 1.024101315766604\n",
            "Loss: 1.025164942727195\n",
            "Loss: 1.070559394986626\n",
            "Loss: 1.067690851166366\n",
            "Loss: 1.000042078937196\n",
            "Loss: 1.0558979215768296\n",
            "Epoch 11\n",
            "Loss: 1.0193635842725084\n",
            "Loss: 1.037315545054608\n",
            "Loss: 1.0708944673097407\n",
            "Loss: 1.0687784408097953\n",
            "Loss: 0.9814970782275367\n",
            "Loss: 1.0604575917680439\n",
            "Epoch 12\n",
            "Loss: 1.0055632095261309\n",
            "Loss: 1.0269370668880784\n",
            "Loss: 1.0959727305657097\n",
            "Loss: 1.0802111137077994\n",
            "Loss: 1.0124629980748971\n",
            "Loss: 1.0519790037056809\n",
            "Epoch 13\n",
            "Loss: 0.9965129112892395\n",
            "Loss: 1.0449418948001028\n",
            "Loss: 1.0598152034340336\n",
            "Loss: 1.0732124443417992\n",
            "Loss: 0.983705030725276\n",
            "Loss: 1.0541829005000487\n",
            "Epoch 14\n",
            "Loss: 0.9979643428274234\n",
            "Loss: 1.016181009856618\n",
            "Loss: 1.0576754327195663\n",
            "Loss: 1.0491978035574792\n",
            "Loss: 1.0036870478153963\n",
            "Loss: 1.0715970180090144\n",
            "Epoch 15\n",
            "Loss: 1.0039692164548197\n",
            "Loss: 1.0256862672711091\n",
            "Loss: 1.0748673570429832\n",
            "Loss: 1.0639120121205141\n",
            "Loss: 0.990646200640802\n",
            "Loss: 1.0591350318637547\n",
            "Epoch 16\n",
            "Loss: 1.0303074744431922\n",
            "Loss: 1.0263962079206963\n",
            "Loss: 1.05118571478611\n",
            "Loss: 1.0601200060875915\n",
            "Loss: 1.007154945835686\n",
            "Loss: 1.0660556622602155\n",
            "Epoch 17\n",
            "Loss: 0.9997573241413487\n",
            "Loss: 1.017281807950955\n",
            "Loss: 1.0719003917107428\n",
            "Loss: 1.0712066381521257\n",
            "Loss: 1.0140223736303104\n",
            "Loss: 1.0598305537097998\n",
            "Epoch 18\n",
            "Loss: 1.0231491674426427\n",
            "Loss: 1.026688614302989\n",
            "Loss: 1.0568955342027642\n",
            "Loss: 1.0458026472236974\n",
            "Loss: 0.9986977488931936\n",
            "Loss: 1.039939372527533\n",
            "Epoch 19\n",
            "Loss: 1.0185266549385814\n",
            "Loss: 1.037698861436572\n",
            "Loss: 1.1254103614597017\n",
            "Loss: 1.0396216996131318\n",
            "Loss: 0.9968394359277285\n",
            "Loss: 1.0512141135027435\n",
            "Epoch 20\n",
            "Loss: 0.9973742218802089\n",
            "Loss: 1.0302461547102435\n",
            "Loss: 1.057287417320184\n",
            "Loss: 1.0553921577197043\n",
            "Loss: 0.9841653373756339\n",
            "Loss: 1.0616715770649323\n",
            "Epoch 21\n",
            "Loss: 1.023141129644468\n",
            "Loss: 1.0217541504710257\n",
            "Loss: 1.044714545760386\n",
            "Loss: 1.0741501359749464\n",
            "Loss: 0.9953137765336473\n",
            "Loss: 1.0626900543165831\n",
            "Epoch 22\n",
            "Loss: 1.0180869391697631\n",
            "Loss: 1.0104315470970087\n",
            "Loss: 1.0666115808957346\n",
            "Loss: 1.063979696947524\n",
            "Loss: 1.001341673072441\n",
            "Loss: 1.0573377663097536\n",
            "Epoch 23\n",
            "Loss: 0.9932766357605296\n",
            "Loss: 1.0068270913847244\n",
            "Loss: 1.0556465335773577\n",
            "Loss: 1.0679963799010324\n",
            "Loss: 1.0284681603841321\n",
            "Loss: 1.0591873308643593\n",
            "Epoch 24\n",
            "Loss: 1.0066492374227805\n",
            "Loss: 1.0343836980375436\n",
            "Loss: 1.0777453219627453\n",
            "Loss: 1.0873788558445119\n",
            "Loss: 1.0220207018949141\n",
            "Loss: 1.0539091518883703\n",
            "Epoch 25\n",
            "Loss: 1.0117631169692531\n",
            "Loss: 1.0156042814635842\n",
            "Loss: 1.060927520391461\n",
            "Loss: 1.0753873833080903\n",
            "Loss: 1.0198660210943418\n",
            "Loss: 1.0472433837099455\n",
            "Epoch 26\n",
            "Loss: 1.0341838770233192\n",
            "Loss: 1.0233491445384442\n",
            "Loss: 1.1102427098987935\n",
            "Loss: 1.0815391719096914\n",
            "Loss: 1.0105236820747523\n",
            "Loss: 1.0562222604449631\n",
            "Epoch 27\n",
            "Loss: 1.0417518512832808\n",
            "Loss: 1.013706320104327\n",
            "Loss: 1.0621616459667749\n",
            "Loss: 1.0478023766387332\n",
            "Loss: 1.016699690529513\n",
            "Loss: 1.0868091389531112\n",
            "Epoch 28\n",
            "Loss: 1.0317445764409765\n",
            "Loss: 1.0228262584216308\n",
            "Loss: 1.058901191558951\n",
            "Loss: 1.0872872300867642\n",
            "Loss: 1.0258744998894636\n",
            "Loss: 1.0776571057931295\n",
            "Epoch 29\n",
            "Loss: 1.0093725291316067\n",
            "Loss: 1.0244573604904155\n",
            "Loss: 1.078930949183685\n",
            "Loss: 1.072680863539483\n",
            "Loss: 1.0294938836523573\n",
            "Loss: 1.072270913514286\n",
            "Epoch 30\n",
            "Loss: 1.0186637728116363\n",
            "Loss: 1.024383983354014\n",
            "Loss: 1.0678427218591549\n",
            "Loss: 1.0618305826747698\n",
            "Loss: 1.0156893241690756\n",
            "Loss: 1.0688689518696348\n",
            "Epoch 31\n",
            "Loss: 1.0191279525319161\n",
            "Loss: 1.0300572948983664\n",
            "Loss: 1.056487438989873\n",
            "Loss: 1.0793127461010117\n",
            "Loss: 1.0058983444182399\n",
            "Loss: 1.0682094522155379\n",
            "Epoch 32\n",
            "Loss: 1.0010223440823818\n",
            "Loss: 1.022476239531923\n",
            "Loss: 1.0913231354645714\n",
            "Loss: 1.054397018787062\n",
            "Loss: 1.0273205766945739\n",
            "Loss: 1.0700499971909703\n",
            "Epoch 33\n",
            "Loss: 1.0114695590419154\n",
            "Loss: 1.0226079380138404\n",
            "Loss: 1.08272801462119\n",
            "Loss: 1.052886015644522\n",
            "Loss: 1.0373576586118407\n",
            "Loss: 1.0797595200073677\n",
            "Epoch 34\n",
            "Loss: 0.9996062742819041\n",
            "Loss: 1.03436755155554\n",
            "Loss: 1.100369056679638\n",
            "Loss: 1.052070873182422\n",
            "Loss: 0.9975927653282061\n",
            "Loss: 1.05831795425255\n",
            "Epoch 35\n",
            "Loss: 1.0221433401624653\n",
            "Loss: 1.0251989763086167\n",
            "Loss: 1.0669008249743779\n",
            "Loss: 1.0509209718805028\n",
            "Loss: 1.0036675936733501\n",
            "Loss: 1.0609526795511819\n",
            "Epoch 36\n",
            "Loss: 1.005586281556797\n",
            "Loss: 1.0275717158427486\n",
            "Loss: 1.0833353775684964\n",
            "Loss: 1.0456172395847605\n",
            "Loss: 1.0003555406285292\n",
            "Loss: 1.0685636180507179\n",
            "Epoch 37\n",
            "Loss: 1.0071128351085221\n",
            "Loss: 1.0229378287198725\n",
            "Loss: 1.0920288537992966\n",
            "Loss: 1.0430827911831804\n",
            "Loss: 1.0080828412111182\n",
            "Loss: 1.0965244335668998\n",
            "Epoch 38\n",
            "Loss: 1.0042699820356766\n",
            "Loss: 1.0271047039331667\n",
            "Loss: 1.0715027055031954\n",
            "Loss: 1.0513496086938052\n",
            "Loss: 0.9930794189388309\n",
            "Loss: 1.030459338973095\n",
            "Epoch 39\n",
            "Loss: 0.9995814709644514\n",
            "Loss: 1.0169447187942589\n",
            "Loss: 1.0903954188690796\n",
            "Loss: 1.0393828520348714\n",
            "Loss: 1.0161953978658298\n",
            "Loss: 1.0449475224894302\n",
            "Epoch 40\n",
            "Loss: 1.0021604685598058\n",
            "Loss: 1.0203282242814535\n",
            "Loss: 1.0534454536776652\n",
            "Loss: 1.0453636289672796\n",
            "Loss: 0.9795679019778173\n",
            "Loss: 1.063901282807117\n",
            "Epoch 41\n",
            "Loss: 1.028692757850611\n",
            "Loss: 1.0176622607274721\n",
            "Loss: 1.0850272845280962\n",
            "Loss: 1.0168929323747546\n",
            "Loss: 1.0410749360949927\n",
            "Loss: 1.0526260406394197\n",
            "Epoch 42\n",
            "Loss: 1.006813918046845\n",
            "Loss: 1.0338527358423966\n",
            "Loss: 1.061991442440108\n",
            "Loss: 1.0317892966282132\n",
            "Loss: 1.0422554576816676\n",
            "Loss: 1.0602705855044618\n",
            "Epoch 43\n",
            "Loss: 1.0128089543706733\n",
            "Loss: 1.0119357900918116\n",
            "Loss: 1.0909107466109444\n",
            "Loss: 1.0059345409409302\n",
            "Loss: 1.0670778240937038\n",
            "Loss: 1.062017117841794\n",
            "Epoch 44\n",
            "Loss: 1.004207833663036\n",
            "Loss: 0.9921892041589943\n",
            "Loss: 1.0651360378553378\n",
            "Loss: 1.0821201605062019\n",
            "Loss: 1.0412029148179813\n",
            "Loss: 1.1103508035623264\n",
            "Epoch 45\n",
            "Loss: 1.0310389323213904\n",
            "Loss: 1.0186892324373236\n",
            "Loss: 1.0822916952660044\n",
            "Loss: 1.0326733464294224\n",
            "Loss: 1.0106003207437289\n",
            "Loss: 1.0767517883785054\n",
            "Epoch 46\n",
            "Loss: 1.0233627388784368\n",
            "Loss: 0.9982374382050474\n",
            "Loss: 1.0889787029096754\n",
            "Loss: 1.0370091585912753\n",
            "Loss: 1.026245209855448\n",
            "Loss: 1.059189027563689\n",
            "Epoch 47\n",
            "Loss: 1.0296562363498323\n",
            "Loss: 0.9875063941673483\n",
            "Loss: 1.085378305649321\n",
            "Loss: 1.0119408210293006\n",
            "Loss: 1.0159898036777986\n",
            "Loss: 1.065219333219153\n",
            "Epoch 48\n",
            "Loss: 1.0033133080995613\n",
            "Loss: 0.9998724505564436\n",
            "Loss: 1.0934903298690304\n",
            "Loss: 1.0321682719680911\n",
            "Loss: 1.042262466680427\n",
            "Loss: 1.054715258314523\n",
            "Epoch 49\n",
            "Loss: 1.0221503622112218\n",
            "Loss: 1.0013179200761129\n",
            "Loss: 1.084007954071941\n",
            "Loss: 1.0453227370768572\n",
            "Loss: 1.0020086607624406\n",
            "Loss: 1.0590414678213151\n",
            "Epoch 50\n",
            "Loss: 1.0171419081135393\n",
            "Loss: 1.0315002150969463\n",
            "Loss: 1.0818458777441244\n",
            "Loss: 1.0466036392735858\n",
            "Loss: 1.017951375096314\n",
            "Loss: 1.0584614943470192\n",
            "Epoch 51\n",
            "Loss: 1.0042675241651111\n",
            "Loss: 1.0212053133300962\n",
            "Loss: 1.1167961283561176\n",
            "Loss: 1.0180451289980685\n",
            "Loss: 0.9999561314567891\n",
            "Loss: 1.1093515689091253\n",
            "Epoch 52\n",
            "Loss: 1.009630035542384\n",
            "Loss: 1.0085158391090188\n",
            "Loss: 1.081474246612082\n",
            "Loss: 1.0585021007391902\n",
            "Loss: 1.0110932515962072\n",
            "Loss: 1.073257670526175\n",
            "Epoch 53\n",
            "Loss: 0.9846361810838453\n",
            "Loss: 1.0129551540668291\n",
            "Loss: 1.1034651049708961\n",
            "Loss: 1.0435839574451604\n",
            "Loss: 1.0023421262667265\n",
            "Loss: 1.0847217119363894\n",
            "Epoch 54\n",
            "Loss: 0.9991177875408239\n",
            "Loss: 0.9544507277480472\n",
            "Loss: 1.1104686661209449\n",
            "Loss: 1.0159212778870914\n",
            "Loss: 0.9975598003345959\n",
            "Loss: 1.067017008957076\n",
            "Epoch 55\n",
            "Loss: 0.9874507259608807\n",
            "Loss: 1.0088950560380523\n",
            "Loss: 1.0994832244743882\n",
            "Loss: 1.0310890901363252\n",
            "Loss: 1.0055587262800378\n",
            "Loss: 1.0636688387893352\n",
            "Epoch 56\n",
            "Loss: 0.9988973652431268\n",
            "Loss: 1.0511766201291302\n",
            "Loss: 1.0952204808962107\n",
            "Loss: 1.0293390977902588\n",
            "Loss: 1.0360051384574822\n",
            "Loss: 1.0706760218150877\n",
            "Epoch 57\n",
            "Loss: 1.009713818121805\n",
            "Loss: 1.0228913493055352\n",
            "Loss: 1.1044397261088594\n",
            "Loss: 1.0351011808589694\n",
            "Loss: 1.0162002406740083\n",
            "Loss: 1.0901134202513376\n",
            "Epoch 58\n",
            "Loss: 1.0136246885750009\n",
            "Loss: 1.0454355559596906\n",
            "Loss: 1.0924974741517495\n",
            "Loss: 1.0327766186139131\n",
            "Loss: 1.0322495533702765\n",
            "Loss: 1.071574129928794\n",
            "Epoch 59\n",
            "Loss: 1.0003418399830486\n",
            "Loss: 1.0314699937312983\n",
            "Loss: 1.1047280636422556\n",
            "Loss: 1.0315402197619972\n",
            "Loss: 1.0388602655214134\n",
            "Loss: 1.0695109011804036\n",
            "Epoch 60\n",
            "Loss: 1.0058823438913687\n",
            "Loss: 1.0229637610768454\n",
            "Loss: 1.0764188622092696\n",
            "Loss: 1.036276436564545\n",
            "Loss: 1.039444993296173\n",
            "Loss: 1.0706922965965568\n",
            "Epoch 61\n",
            "Loss: 1.0024650954561374\n",
            "Loss: 0.9957517538981426\n",
            "Loss: 1.0620081543972977\n",
            "Loss: 1.0429528695767742\n",
            "Loss: 1.0254401024078437\n",
            "Loss: 1.066556349257828\n",
            "Epoch 62\n",
            "Loss: 1.0067708032934028\n",
            "Loss: 1.0006033916279369\n",
            "Loss: 1.0872317850132627\n",
            "Loss: 1.0498769391350562\n",
            "Loss: 1.035834585720886\n",
            "Loss: 1.0618770012979168\n",
            "Epoch 63\n",
            "Loss: 1.017869408178617\n",
            "Loss: 1.0472395499166551\n",
            "Loss: 1.087682298136491\n",
            "Loss: 1.039684167982982\n",
            "Loss: 1.000692785976026\n",
            "Loss: 1.0628615082973059\n",
            "Epoch 64\n",
            "Loss: 1.0163201997892921\n",
            "Loss: 1.0054922238952833\n",
            "Loss: 1.0951659470357615\n",
            "Loss: 1.0291609116473917\n",
            "Loss: 1.0087208648827484\n",
            "Loss: 1.053115082376189\n",
            "Epoch 65\n",
            "Loss: 1.0189762676940093\n",
            "Loss: 1.0611018824074194\n",
            "Loss: 1.0750338737530754\n",
            "Loss: 1.0351819399659934\n",
            "Loss: 1.0281160305297816\n",
            "Loss: 1.0631855178929666\n",
            "Epoch 66\n",
            "Loss: 0.9910798442500754\n",
            "Loss: 0.9800910650497693\n",
            "Loss: 1.1070078801907417\n",
            "Loss: 1.0576218512710422\n",
            "Loss: 1.006938988830706\n",
            "Loss: 1.0741412433220328\n",
            "Epoch 67\n",
            "Loss: 1.0250235272532568\n",
            "Loss: 1.0346054006935508\n",
            "Loss: 1.0941574297859624\n",
            "Loss: 1.0459809378864735\n",
            "Loss: 1.057089681288581\n",
            "Loss: 1.105958795580078\n",
            "Epoch 68\n",
            "Loss: 1.003980332937705\n",
            "Loss: 1.0281964996483235\n",
            "Loss: 1.1017084103330006\n",
            "Loss: 1.0539348541326745\n",
            "Loss: 1.0144037433579256\n",
            "Loss: 1.061438138405118\n",
            "Epoch 69\n",
            "Loss: 1.0110556512353064\n",
            "Loss: 1.031067279735753\n",
            "Loss: 1.0978387292672038\n",
            "Loss: 1.047531222057407\n",
            "Loss: 1.0601858806934779\n",
            "Loss: 1.0522631413389743\n",
            "Epoch 70\n",
            "Loss: 1.025808681991622\n",
            "Loss: 1.0341803821679796\n",
            "Loss: 1.0856656935630786\n",
            "Loss: 1.0405369777694413\n",
            "Loss: 1.0658592857905758\n",
            "Loss: 1.0649778033036479\n",
            "Epoch 71\n",
            "Loss: 1.0186056273077768\n",
            "Loss: 1.0397098225656385\n",
            "Loss: 1.1079353036969688\n",
            "Loss: 1.0416709553144046\n",
            "Loss: 1.0384899110851096\n",
            "Loss: 1.0767492797801181\n",
            "Epoch 72\n",
            "Loss: 1.047802369663434\n",
            "Loss: 1.074222947473118\n",
            "Loss: 1.0733234933487474\n",
            "Loss: 1.0500690102710302\n",
            "Loss: 1.0150657395964928\n",
            "Loss: 1.0425595849088212\n",
            "Epoch 73\n",
            "Loss: 1.0397164774187464\n",
            "Loss: 1.0252640093989964\n",
            "Loss: 1.0926539495093197\n",
            "Loss: 1.0368702080130512\n",
            "Loss: 1.0406247549432706\n",
            "Loss: 1.0737182532881038\n",
            "Epoch 74\n",
            "Loss: 1.0033900266193072\n",
            "Loss: 0.9926438430930393\n",
            "Loss: 1.0948413973015523\n",
            "Loss: 1.0290000768171486\n",
            "Loss: 1.0346621292990053\n",
            "Loss: 1.0816754169263922\n",
            "Epoch 75\n",
            "Loss: 1.0387324214126308\n",
            "Loss: 1.023536067502898\n",
            "Loss: 1.095332875178694\n",
            "Loss: 1.0478563603136801\n",
            "Loss: 1.0531999675885801\n",
            "Loss: 1.0574965168733077\n",
            "Epoch 76\n",
            "Loss: 0.9972246295326983\n",
            "Loss: 1.0096406125409567\n",
            "Loss: 1.0958120176472765\n",
            "Loss: 1.0526676716498296\n",
            "Loss: 1.049891447544716\n",
            "Loss: 1.053417768666577\n",
            "Epoch 77\n",
            "Loss: 1.015447236251016\n",
            "Loss: 1.0143408968463166\n",
            "Loss: 1.0655265057542476\n",
            "Loss: 1.006071543464203\n",
            "Loss: 1.028966998131758\n",
            "Loss: 1.061464598872774\n",
            "Epoch 78\n",
            "Loss: 1.019515485620554\n",
            "Loss: 1.0155569512506095\n",
            "Loss: 1.0680253423909167\n",
            "Loss: 1.0452340863552287\n",
            "Loss: 1.0280958329082703\n",
            "Loss: 1.0642373550753008\n",
            "Epoch 79\n",
            "Loss: 1.0284747808560637\n",
            "Loss: 1.0213760352146468\n",
            "Loss: 1.0709385087332959\n",
            "Loss: 1.0309667411834627\n",
            "Loss: 1.0501309869528197\n",
            "Loss: 1.0665987301850106\n",
            "Epoch 80\n",
            "Loss: 1.0328369490200269\n",
            "Loss: 1.0410467988049323\n",
            "Loss: 1.0692398850500469\n",
            "Loss: 1.0271707440895947\n",
            "Loss: 1.0821321362086458\n",
            "Loss: 1.0421755778531714\n",
            "Epoch 81\n",
            "Loss: 1.0150981331337872\n",
            "Loss: 1.0364360078144275\n",
            "Loss: 1.077477364682433\n",
            "Loss: 1.0309426630012668\n",
            "Loss: 1.056935547370816\n",
            "Loss: 1.0391214758878762\n",
            "Epoch 82\n",
            "Loss: 1.0135952468356688\n",
            "Loss: 1.0276416344292993\n",
            "Loss: 1.0632258986258036\n",
            "Loss: 1.0146707146633682\n",
            "Loss: 1.0416310499396646\n",
            "Loss: 1.029705826078001\n",
            "Epoch 83\n",
            "Loss: 1.016324829867103\n",
            "Loss: 1.0174844251905177\n",
            "Loss: 1.0714649809031882\n",
            "Loss: 1.0413261039064805\n",
            "Loss: 1.0338641880036432\n",
            "Loss: 1.057014140312872\n",
            "Epoch 84\n",
            "Loss: 1.0476047462458016\n",
            "Loss: 1.005554741488222\n",
            "Loss: 1.0671256495235946\n",
            "Loss: 1.0384435341172238\n",
            "Loss: 1.0742683802885638\n",
            "Loss: 1.0637058297764694\n",
            "Epoch 85\n",
            "Loss: 1.082958979676837\n",
            "Loss: 0.998859804379679\n",
            "Loss: 1.0914088592976436\n",
            "Loss: 1.0542856876299231\n",
            "Loss: 1.049374946252163\n",
            "Loss: 1.0585577274480613\n",
            "Epoch 86\n",
            "Loss: 1.0581084881213678\n",
            "Loss: 1.0180167180974165\n",
            "Loss: 1.0689388435058396\n",
            "Loss: 1.0392171856959636\n",
            "Loss: 1.0305563579419097\n",
            "Loss: 1.057264169945634\n",
            "Epoch 87\n",
            "Loss: 1.0337483125883637\n",
            "Loss: 1.0112936835499169\n",
            "Loss: 1.0890781007901025\n",
            "Loss: 1.0405882291084088\n",
            "Loss: 1.0523985635484823\n",
            "Loss: 1.0634081947222471\n",
            "Epoch 88\n",
            "Loss: 1.0400624208496934\n",
            "Loss: 1.0158139118668608\n",
            "Loss: 1.076382843775804\n",
            "Loss: 1.0495319278937738\n",
            "Loss: 1.0419208528617587\n",
            "Loss: 1.0442622763811584\n",
            "Epoch 89\n",
            "Loss: 1.006404000739809\n",
            "Loss: 1.0176873775409017\n",
            "Loss: 1.0624277569249991\n",
            "Loss: 1.0462527080737039\n",
            "Loss: 1.0680538264144004\n",
            "Loss: 1.0478244179150469\n",
            "Epoch 90\n",
            "Loss: 1.0016999582945536\n",
            "Loss: 1.0227185953867246\n",
            "Loss: 1.056787952764213\n",
            "Loss: 1.049318508637564\n",
            "Loss: 1.0420192617111739\n",
            "Loss: 1.058254056242579\n",
            "Epoch 91\n",
            "Loss: 1.0125490881677928\n",
            "Loss: 0.9988878215611083\n",
            "Loss: 1.0566007230460919\n",
            "Loss: 1.061082773369904\n",
            "Loss: 1.0394481086237843\n",
            "Loss: 1.0522395208884832\n",
            "Epoch 92\n",
            "Loss: 1.0066782451128071\n",
            "Loss: 0.9974170646704521\n",
            "Loss: 1.0624375343340862\n",
            "Loss: 1.0280638399001263\n",
            "Loss: 1.0553592749693812\n",
            "Loss: 1.0436871369054472\n",
            "Epoch 93\n",
            "Loss: 1.009584238735206\n",
            "Loss: 1.0270876307661165\n",
            "Loss: 1.077007778092491\n",
            "Loss: 1.0392810714059975\n",
            "Loss: 1.0721981318627631\n",
            "Loss: 1.0454376564912637\n",
            "Epoch 94\n",
            "Loss: 1.0459961306752552\n",
            "Loss: 1.0100687294086512\n",
            "Loss: 1.0658887750860253\n",
            "Loss: 1.0404674614091292\n",
            "Loss: 1.0350878547552758\n",
            "Loss: 1.0554887607880017\n",
            "Epoch 95\n",
            "Loss: 1.049765944267437\n",
            "Loss: 0.9947081469388599\n",
            "Loss: 1.0695506719593668\n",
            "Loss: 1.0350167623568114\n",
            "Loss: 1.0468049117246898\n",
            "Loss: 1.0286219468961364\n",
            "Epoch 96\n",
            "Loss: 1.0353287416342507\n",
            "Loss: 0.9979430135703555\n",
            "Loss: 1.0560126470218931\n",
            "Loss: 1.0500270878132285\n",
            "Loss: 1.0738557520067755\n",
            "Loss: 1.0617761935183145\n",
            "Epoch 97\n",
            "Loss: 1.0191921246140239\n",
            "Loss: 0.9927016333145562\n",
            "Loss: 1.0771944406561547\n",
            "Loss: 1.040100030975799\n",
            "Loss: 1.0460575323759294\n",
            "Loss: 1.0576539844998034\n",
            "Epoch 98\n",
            "Loss: 1.0115683599503416\n",
            "Loss: 0.9895612101968684\n",
            "Loss: 1.0905642553131056\n",
            "Loss: 1.0478642843802206\n",
            "Loss: 1.0304787851173551\n",
            "Loss: 1.054179182275423\n",
            "Epoch 99\n",
            "Loss: 1.0676461568468631\n",
            "Loss: 0.9862648000752907\n",
            "Loss: 1.0537480332597897\n",
            "Loss: 1.017591412941729\n",
            "Loss: 1.0595473831045528\n",
            "Loss: 1.051989610046918\n",
            "[0 0 2 ... 2 1 2]\n",
            "0.4074659292909342\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "batch_size = np.shape(train_data)[1]\n",
        "epochs = 200\n",
        "learning_rate = 0.001\n",
        "valid_epoch = 1\n",
        "\n",
        "model = torchNeuralNetwork(np.shape(train_data)[1], 3)\n",
        "loss_fn = nn.BCELoss()\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
        "best_loss = 10\n",
        "\n",
        "for e in range(epochs):\n",
        "  model.train()\n",
        "  print(\"Epoch {}\".format(e))\n",
        "  for batch in range(np.shape(train_data)[0]//batch_size):\n",
        "    pred = model(torch.Tensor(train_data[batch*batch_size : (batch+1)*batch_size]))\n",
        "    loss = loss_fn(pred, torch.Tensor(train_targets[batch*batch_size : (batch+1)*batch_size]))\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "    optimizer.zero_grad()\n",
        "    if(batch % 100 == 0):\n",
        "      loss, current = loss.item(), batch * batch_size\n",
        "      print(f\"loss: {loss:>7f}  [{current:>5d}/{np.shape(train_targets)[0]:>5d}]\")\n",
        "  if(e % valid_epoch == 0):\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "      pred = model(torch.Tensor(val_data))\n",
        "      loss = loss_fn(pred, torch.Tensor(val_targets)[0])\n",
        "      print(\"Validation loss: {:.7f}\".format(loss.item()))\n",
        "      if loss.item() > best_loss:\n",
        "        break\n",
        "      best_loss = loss.item()\n",
        "  print()\n",
        "  # print(f\"loss: {loss:>7f}\")\n",
        "\n",
        "model.eval()\n",
        "with torch.no_grad():\n",
        "  pred = model(torch.Tensor(test_data))\n",
        "\n",
        "  print(pred)\n",
        "  print(np.sum(test_targets[range(np.shape(test_targets)[0]), np.argmax(np.array(pred), axis=1)])/np.shape(test_targets)[0])\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FX2UciBML2gN",
        "outputId": "f483b250-6b43-4272-d9a6-dedc71d0a2b9"
      },
      "execution_count": 227,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 0\n",
            "loss: 0.638477  [    0/16875]\n",
            "loss: 0.629087  [ 3400/16875]\n",
            "loss: 0.637131  [ 6800/16875]\n",
            "loss: 0.610273  [10200/16875]\n",
            "loss: 0.600855  [13600/16875]\n",
            "Validation loss: 0.6162647\n",
            "\n",
            "Epoch 1\n",
            "loss: 0.625421  [    0/16875]\n",
            "loss: 0.599668  [ 3400/16875]\n",
            "loss: 0.635381  [ 6800/16875]\n",
            "loss: 0.603334  [10200/16875]\n",
            "loss: 0.599370  [13600/16875]\n",
            "Validation loss: 0.6154556\n",
            "\n",
            "Epoch 2\n",
            "loss: 0.623961  [    0/16875]\n",
            "loss: 0.598230  [ 3400/16875]\n",
            "loss: 0.634221  [ 6800/16875]\n",
            "loss: 0.604294  [10200/16875]\n",
            "loss: 0.599079  [13600/16875]\n",
            "Validation loss: 0.6152219\n",
            "\n",
            "Epoch 3\n",
            "loss: 0.622742  [    0/16875]\n",
            "loss: 0.598075  [ 3400/16875]\n",
            "loss: 0.633393  [ 6800/16875]\n",
            "loss: 0.605098  [10200/16875]\n",
            "loss: 0.598907  [13600/16875]\n",
            "Validation loss: 0.6151251\n",
            "\n",
            "Epoch 4\n",
            "loss: 0.621953  [    0/16875]\n",
            "loss: 0.598074  [ 3400/16875]\n",
            "loss: 0.632903  [ 6800/16875]\n",
            "loss: 0.605742  [10200/16875]\n",
            "loss: 0.598800  [13600/16875]\n",
            "Validation loss: 0.6150464\n",
            "\n",
            "Epoch 5\n",
            "loss: 0.621337  [    0/16875]\n",
            "loss: 0.598134  [ 3400/16875]\n",
            "loss: 0.632704  [ 6800/16875]\n",
            "loss: 0.606277  [10200/16875]\n",
            "loss: 0.598723  [13600/16875]\n",
            "Validation loss: 0.6149700\n",
            "\n",
            "Epoch 6\n",
            "loss: 0.620826  [    0/16875]\n",
            "loss: 0.598229  [ 3400/16875]\n",
            "loss: 0.632768  [ 6800/16875]\n",
            "loss: 0.606698  [10200/16875]\n",
            "loss: 0.598635  [13600/16875]\n",
            "Validation loss: 0.6148875\n",
            "\n",
            "Epoch 7\n",
            "loss: 0.620419  [    0/16875]\n",
            "loss: 0.598357  [ 3400/16875]\n",
            "loss: 0.633087  [ 6800/16875]\n",
            "loss: 0.606998  [10200/16875]\n",
            "loss: 0.598503  [13600/16875]\n",
            "Validation loss: 0.6147594\n",
            "\n",
            "Epoch 8\n",
            "loss: 0.620142  [    0/16875]\n",
            "loss: 0.598514  [ 3400/16875]\n",
            "loss: 0.633654  [ 6800/16875]\n",
            "loss: 0.607183  [10200/16875]\n",
            "loss: 0.598297  [13600/16875]\n",
            "Validation loss: 0.6145974\n",
            "\n",
            "Epoch 9\n",
            "loss: 0.620024  [    0/16875]\n",
            "loss: 0.598692  [ 3400/16875]\n",
            "loss: 0.634437  [ 6800/16875]\n",
            "loss: 0.607282  [10200/16875]\n",
            "loss: 0.597992  [13600/16875]\n",
            "Validation loss: 0.6144064\n",
            "\n",
            "Epoch 10\n",
            "loss: 0.620085  [    0/16875]\n",
            "loss: 0.598905  [ 3400/16875]\n",
            "loss: 0.635338  [ 6800/16875]\n",
            "loss: 0.607336  [10200/16875]\n",
            "loss: 0.597564  [13600/16875]\n",
            "Validation loss: 0.6141778\n",
            "\n",
            "Epoch 11\n",
            "loss: 0.620317  [    0/16875]\n",
            "loss: 0.599225  [ 3400/16875]\n",
            "loss: 0.636166  [ 6800/16875]\n",
            "loss: 0.607346  [10200/16875]\n",
            "loss: 0.596998  [13600/16875]\n",
            "Validation loss: 0.6139368\n",
            "\n",
            "Epoch 12\n",
            "loss: 0.620657  [    0/16875]\n",
            "loss: 0.599759  [ 3400/16875]\n",
            "loss: 0.636669  [ 6800/16875]\n",
            "loss: 0.607259  [10200/16875]\n",
            "loss: 0.596311  [13600/16875]\n",
            "Validation loss: 0.6136975\n",
            "\n",
            "Epoch 13\n",
            "loss: 0.620978  [    0/16875]\n",
            "loss: 0.600586  [ 3400/16875]\n",
            "loss: 0.636648  [ 6800/16875]\n",
            "loss: 0.607008  [10200/16875]\n",
            "loss: 0.595533  [13600/16875]\n",
            "Validation loss: 0.6135523\n",
            "\n",
            "Epoch 14\n",
            "loss: 0.621127  [    0/16875]\n",
            "loss: 0.601700  [ 3400/16875]\n",
            "loss: 0.636083  [ 6800/16875]\n",
            "loss: 0.606572  [10200/16875]\n",
            "loss: 0.594673  [13600/16875]\n",
            "Validation loss: 0.6134698\n",
            "\n",
            "Epoch 15\n",
            "loss: 0.621010  [    0/16875]\n",
            "loss: 0.603019  [ 3400/16875]\n",
            "loss: 0.635117  [ 6800/16875]\n",
            "loss: 0.605982  [10200/16875]\n",
            "loss: 0.593707  [13600/16875]\n",
            "Validation loss: 0.6134348\n",
            "\n",
            "Epoch 16\n",
            "loss: 0.620597  [    0/16875]\n",
            "loss: 0.604427  [ 3400/16875]\n",
            "loss: 0.633943  [ 6800/16875]\n",
            "loss: 0.605290  [10200/16875]\n",
            "loss: 0.592616  [13600/16875]\n",
            "Validation loss: 0.6134483\n",
            "tensor([[0.2376, 0.3419, 0.4206],\n",
            "        [0.1945, 0.2085, 0.5969],\n",
            "        [0.4462, 0.3996, 0.1541],\n",
            "        ...,\n",
            "        [0.4498, 0.3146, 0.2357],\n",
            "        [0.3025, 0.2626, 0.4349],\n",
            "        [0.3510, 0.4352, 0.2138]])\n",
            "0.43729014418329054\n"
          ]
        }
      ]
    }
  ]
}