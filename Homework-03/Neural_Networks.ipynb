{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNNBAYfnbMq9nzzfs0M4jfI",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/csch7/CSCI-4170/blob/main/Homework-03/Neural_Networks.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Neural Networks\n",
        "### Author: Colin Scherer\n",
        "\n",
        "For this assignment, I've chosen a dataset of diabetes patients -- I'm trying to determine whether each patient will be readmitted to the hospital, and how soon. Although that sounds like a mix of classification and regression, this is in reality a multi-class classification problem with 3 classes: readmitted within 30 days, readmitted after 30 days, and not readmitted. As this dataset is very messy, I will have to do extensive preprocessing: any feature with more than 2% of values missing I will just remove -- this isn't too bad, since this only impacts features which intuitively wouldn't have much of an impact on the problem. I additionally convert all strings to an integer I thought made sense. Performing one-hot encoding on the categorical features would also be advised, but I ran out of time to do so. The targets of this dataset are also very imbalanced, with only 10% of the dataset being classified as one of 3 targets, and 60% of the dataset being classified as another. To solve this issue, I will downsample the larger portions randomly. I can afford to do this since the raw dataset has about 100k samples.\n",
        "\n",
        "Even with these problems addressed, the dataset in general is messy, with plenty of missing data and unbalanced variables. For example, I am generalizing \"NO\" for \"readmitted\" to mean the patient was never readmitted. In reality, a \"NO\" means there was no record of readmission -- many of these could just be records that were lost. Due to the bad quality of data, I'm not expecting to get high accuracies -- any accuracies greater than ~40% will be considered a win."
      ],
      "metadata": {
        "id": "hS39QL0UvLyx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd\n",
        "import re\n",
        "from google.colab import drive\n",
        "\n",
        "def repl_age(age: str) -> int:\n",
        "  return int(re.findall(r'\\d+', age)[0]) # Converts age string into an integer by taking the lower bound of the range.\n",
        "\n",
        "drive.mount('/content/drive')\n",
        "df = pd.read_csv(\"/content/drive/MyDrive/Colab Notebooks/diabetic_data.csv\")\n",
        "\n",
        "for c in df.columns:\n",
        "  pctUnknown = np.sum([df[c] == '?'])/df[c].shape[0]\n",
        "  if(pctUnknown > 0.02):\n",
        "    df.drop(c, axis = 1, inplace = True)\n",
        "  else:\n",
        "    df = df[df[c] != '?']\n",
        "\n",
        "# Transform strings into integers\n",
        "df.drop('diag_1', axis = 1, inplace = True)\n",
        "df.drop('diag_2', axis = 1, inplace = True)\n",
        "df.drop('diag_3', axis = 1, inplace = True)\n",
        "df.replace('Male', 0, inplace = True)\n",
        "df.replace('Female', 1, inplace = True)\n",
        "df = df[df['gender'] != 'Unknown/Invalid']\n",
        "df.replace('No', 0, inplace = True)\n",
        "df.replace('NO', 2, inplace = True)\n",
        "df.replace('>30', 1, inplace = True)\n",
        "df.replace('<30', 0, inplace = True)\n",
        "df.replace('Down', 1, inplace = True)\n",
        "df.replace('Steady', 2, inplace = True)\n",
        "df.replace('Up', 3, inplace = True)\n",
        "df.replace('Yes', 1, inplace = True)\n",
        "df.replace('Ch', 1, inplace = True)\n",
        "df.replace('Norm', 1, inplace = True)\n",
        "df.replace('>7', 2, inplace = True)\n",
        "df.replace('>8', 3, inplace = True)\n",
        "df.replace('>200', 2, inplace = True)\n",
        "df.replace('>300', 3, inplace = True)\n",
        "df.fillna(0, inplace = True)\n",
        "df['age'] = df['age'].apply(repl_age)\n",
        "\n",
        "tars = df['readmitted']\n",
        "df.drop('readmitted', axis = 1, inplace = True)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "P7re2mfwpgZN",
        "outputId": "affbd29a-a488-4e73-b9b0-4658ff743fac"
      },
      "execution_count": 316,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-316-d281ce0916d6>:27: FutureWarning: Downcasting behavior in `replace` is deprecated and will be removed in a future version. To retain the old behavior, explicitly call `result.infer_objects(copy=False)`. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n",
            "  df.replace('No', 0, inplace = True)\n",
            "<ipython-input-316-d281ce0916d6>:30: FutureWarning: Downcasting behavior in `replace` is deprecated and will be removed in a future version. To retain the old behavior, explicitly call `result.infer_objects(copy=False)`. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n",
            "  df.replace('<30', 0, inplace = True)\n",
            "<ipython-input-316-d281ce0916d6>:32: FutureWarning: Downcasting behavior in `replace` is deprecated and will be removed in a future version. To retain the old behavior, explicitly call `result.infer_objects(copy=False)`. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n",
            "  df.replace('Steady', 2, inplace = True)\n",
            "<ipython-input-316-d281ce0916d6>:33: FutureWarning: Downcasting behavior in `replace` is deprecated and will be removed in a future version. To retain the old behavior, explicitly call `result.infer_objects(copy=False)`. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n",
            "  df.replace('Up', 3, inplace = True)\n",
            "<ipython-input-316-d281ce0916d6>:34: FutureWarning: Downcasting behavior in `replace` is deprecated and will be removed in a future version. To retain the old behavior, explicitly call `result.infer_objects(copy=False)`. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n",
            "  df.replace('Yes', 1, inplace = True)\n",
            "<ipython-input-316-d281ce0916d6>:35: FutureWarning: Downcasting behavior in `replace` is deprecated and will be removed in a future version. To retain the old behavior, explicitly call `result.infer_objects(copy=False)`. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n",
            "  df.replace('Ch', 1, inplace = True)\n",
            "<ipython-input-316-d281ce0916d6>:38: FutureWarning: Downcasting behavior in `replace` is deprecated and will be removed in a future version. To retain the old behavior, explicitly call `result.infer_objects(copy=False)`. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n",
            "  df.replace('>8', 3, inplace = True)\n",
            "<ipython-input-316-d281ce0916d6>:40: FutureWarning: Downcasting behavior in `replace` is deprecated and will be removed in a future version. To retain the old behavior, explicitly call `result.infer_objects(copy=False)`. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n",
            "  df.replace('>300', 3, inplace = True)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from statsmodels.stats.outliers_influence import variance_inflation_factor\n",
        "\n",
        "dropped_features = []\n",
        "\n",
        "for i, feature in enumerate(df.columns):\n",
        "  vif = variance_inflation_factor(df, i)\n",
        "  print(\"{f}:\\t\\t\\t{v}\".format(f=feature, v=vif))\n",
        "  if vif > 5 or np.isnan(vif): # Remove any features with vif > 5.\n",
        "    dropped_features.append(feature)\n",
        "\n",
        "df.drop(dropped_features, axis = 1, inplace = True)\n",
        "df.drop('patient_nbr', axis = 1, inplace = True)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DCG4toEO4IoF",
        "outputId": "f6ad6754-f18c-4232-e3e3-3d007d5d1064"
      },
      "execution_count": 318,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "patient_nbr:\t\t\t2.6187727219405685\n",
            "gender:\t\t\t1.9767106176361124\n",
            "admission_type_id:\t\t\t3.0537541754831503\n",
            "discharge_disposition_id:\t\t\t1.5493688982755114\n",
            "admission_source_id:\t\t\t2.9887910431030305\n",
            "time_in_hospital:\t\t\t3.1912202094514175\n",
            "num_procedures:\t\t\t1.7395012916504649\n",
            "number_outpatient:\t\t\t1.1180938194445622\n",
            "number_emergency:\t\t\t1.1414915515245831\n",
            "number_inpatient:\t\t\t1.3781828685044712\n",
            "max_glu_serum:\t\t\t1.3918876293956812\n",
            "A1Cresult:\t\t\t1.1946062676438023\n",
            "metformin:\t\t\t1.4459896309297542\n",
            "repaglinide:\t\t\t1.0325326059033992\n",
            "nateglinide:\t\t\t1.0142990577027875\n",
            "chlorpropamide:\t\t\t1.0018860873801498\n",
            "glimepiride:\t\t\t1.110445304801677\n",
            "acetohexamide:\t\t\t1.0002580291888405\n",
            "glipizide:\t\t\t1.2615416515890447\n",
            "glyburide:\t\t\t1.242272289756267\n",
            "tolbutamide:\t\t\t1.0005351101108333\n",
            "pioglitazone:\t\t\t1.1547646225920742\n",
            "rosiglitazone:\t\t\t1.135545478683643\n",
            "acarbose:\t\t\t1.0073255839623099\n",
            "miglitol:\t\t\t1.0014137360291113\n",
            "troglitazone:\t\t\t1.0003599762364608\n",
            "tolazamide:\t\t\t1.0010978953284206\n",
            "insulin:\t\t\t2.687638722216703\n",
            "glyburide-metformin:\t\t\t1.0162265106198174\n",
            "glipizide-metformin:\t\t\t1.001283031730118\n",
            "glimepiride-pioglitazone:\t\t\t1.0000869927932896\n",
            "metformin-rosiglitazone:\t\t\t1.0002543804383133\n",
            "metformin-pioglitazone:\t\t\t1.0001962322656688\n",
            "change:\t\t\t3.59063075625552\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def load_dataset(dat, tar):\n",
        "  vals, cts = np.unique(tar, return_counts=True)\n",
        "  min_val = vals[np.argmin(cts)]\n",
        "  min_ct = min(cts)\n",
        "  new_dat = dat[tar == min_val]\n",
        "  new_tar = tar[tar == min_val]\n",
        "  for i, v in enumerate(vals):\n",
        "    if v != min_val:\n",
        "      indices = list(np.where(tar == v))[0]\n",
        "      new_ind = np.random.choice(indices, min_ct)\n",
        "      new_dat = np.append(new_dat, dat[new_ind], axis=0)\n",
        "      new_tar = np.append(new_tar, tar[new_ind])\n",
        "  return new_dat, new_tar\n",
        "\n",
        "\n",
        "def ReLu(z):\n",
        "  return np.maximum(z, 0)\n",
        "\n",
        "\n",
        "\n",
        "class myNeuralNetwork:\n",
        "  def __init__(self, train_data, train_targets, epochs, num_hidden_layers=2, hidden_dim=16, lr=0.01, batch_size = 32):\n",
        "    self.batch_size = batch_size\n",
        "    self.train_data = train_data\n",
        "    self.train_targets = train_targets\n",
        "    self.epochs = epochs\n",
        "    self.learning_rate = lr\n",
        "    self.hidden_layers = num_hidden_layers\n",
        "    self.weights = [np.random.uniform(-0.1, 0.1, (np.shape(train_data)[1], hidden_dim))]\n",
        "    self.biases = [np.random.uniform(-0.1, 0.1, (hidden_dim, 1))]\n",
        "    for i in range(1, num_hidden_layers):\n",
        "      self.weights.append(np.random.uniform(-0.1, 0.1, (hidden_dim, hidden_dim)))\n",
        "      self.biases.append(np.random.uniform(-0.1, 0.1, (hidden_dim, 1)))\n",
        "    self.weights.append(np.random.uniform(-0.1, 0.1, (hidden_dim, np.shape(train_targets)[1])))\n",
        "    self.biases.append(np.random.uniform(-0.1, 0.1, (np.shape(train_targets)[1], 1)))\n",
        "\n",
        "  def forward(self, data):\n",
        "    z = []\n",
        "    a = [data.T]\n",
        "    for l in range(self.hidden_layers+1):\n",
        "      z.append(np.matmul(self.weights[l].T, a[l]) + self.biases[l])\n",
        "      # print(a[-1])\n",
        "      if(l != self.hidden_layers):\n",
        "        a.append(ReLu(z[l]))\n",
        "      else:\n",
        "        a.append(np.exp(z[l])/np.sum(np.exp(z[l]), axis = 0))\n",
        "    return a, z\n",
        "\n",
        "  def backward(self, a, z, tars):\n",
        "    deltas = [0] * (self.hidden_layers+1)\n",
        "    # Calculate gradient for the last layer. The derivative of softmax is given by p_i - y_i\n",
        "    deltas[-1] = (a[-1]-tars.T).T\n",
        "    for l in range(self.hidden_layers-1, -1, -1):\n",
        "      deltas[l] = np.matmul(deltas[l+1], self.weights[l+1].T)*(np.where(z[l] > 0, 1, 0)).T\n",
        "    return deltas\n",
        "\n",
        "\n",
        "  def train(self, val_data, val_tar, val_epoch):\n",
        "    N = np.shape(self.train_data)[0]\n",
        "    best_cost = 10\n",
        "    for e in range(self.epochs):\n",
        "      print(\"Epoch {}\".format(e))\n",
        "      for batch in range(np.shape(self.train_data)[0]//self.batch_size):\n",
        "        a, z = self.forward(self.train_data[self.batch_size*batch:self.batch_size*(batch+1)])\n",
        "        dels = self.backward(a, z, self.train_targets[self.batch_size*batch:self.batch_size*(batch+1)])\n",
        "\n",
        "        for l in range(self.hidden_layers+1):\n",
        "          self.weights[l] -= self.learning_rate*(np.matmul(dels[l].T, a[l].T)).T\n",
        "          self.biases[l] = self.biases[l]-self.learning_rate*np.array([np.mean(dels[l].T, axis=1)]).T\n",
        "        if(batch % 100 == 0):\n",
        "          print(\"Loss: {:.7f}\".format(self.cost(a[-1], self.train_targets[self.batch_size*batch:self.batch_size*(batch+1)])))\n",
        "      if(e % val_epoch == 0):\n",
        "        a, _ = self.forward(val_data)\n",
        "        cst = self.cost(a[-1], val_tar)\n",
        "        print(\"Validation loss: {:.7f}\".format(cst))\n",
        "        if(cst > best_cost):\n",
        "          break\n",
        "        best_cost = cst\n",
        "      print()\n",
        "\n",
        "\n",
        "\n",
        "  def cost(self, preds, tars):\n",
        "    return np.sum(-np.log(preds.T[range(np.shape(preds)[1]), np.argmax(tars, axis=1)]))/np.shape(preds)[1]\n",
        "\n",
        "  def predict(self, data):\n",
        "    probs, _ = self.forward(data)\n",
        "    probs = probs[-1]\n",
        "    return np.argmax(probs.T, axis=1)\n"
      ],
      "metadata": {
        "id": "B_J4_eDLqgt3"
      },
      "execution_count": 350,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "For implementing a 2-layer neural network with pytorch, I closely followed [this tutorial](https://pytorch.org/tutorials/beginner/basics/buildmodel_tutorial.html)."
      ],
      "metadata": {
        "id": "6Os0bZxnsmBA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from torch import nn\n",
        "from torch.utils.data import DataLoader\n",
        "from torchvision import datasets, transforms\n",
        "\n",
        "class torchNeuralNetwork(nn.Module):\n",
        "  def __init__(self, num_features, num_classes, num_hidden_layers=2, hidden_dim=16):\n",
        "    super().__init__()\n",
        "    self.layer_list = [nn.BatchNorm1d(num_features), nn.Linear(num_features, hidden_dim), nn.Tanh()]\n",
        "    for l in range(1, num_hidden_layers):\n",
        "      self.layer_list.append(nn.Linear(hidden_dim, hidden_dim))\n",
        "      self.layer_list.append(nn.Tanh())\n",
        "    self.layer_list.append(nn.Linear(hidden_dim, num_classes))\n",
        "    self.layer_list.append(nn.Softmax())\n",
        "    self.layers = nn.Sequential(*self.layer_list)\n",
        "\n",
        "  def forward(self, x):\n",
        "    return self.layers(x)\n"
      ],
      "metadata": {
        "id": "-L8QNgkBI1uT"
      },
      "execution_count": 274,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "For my training-validation-test split, I've chosen to take half of my dataset for training and sample validation and test sets at a 3:1:1 train:valid:test ratio from the rest of the dataset."
      ],
      "metadata": {
        "id": "YgEedrVVz3y5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "dat = np.array(df)\n",
        "tar = np.array(tars)\n",
        "dat, tar = load_dataset(dat, tar)\n",
        "print(np.shape(dat))\n",
        "N = np.shape(dat)[0]\n",
        "\n",
        "pct_train = 0.5\n",
        "pct_val = pct_train + pct_train*0.3\n",
        "pct_test = pct_val + pct_train*0.3\n",
        "val_epoch = 1\n",
        "indices = np.arange(N)\n",
        "np.random.shuffle(indices)\n",
        "\n",
        "targets = np.zeros((N, 3))\n",
        "targets[range(N), tar] = 1\n",
        "\n",
        "train_data = dat[indices[:int(pct_train*N)]]\n",
        "train_targets = targets[indices[:int(pct_train*N)]]\n",
        "val_data = dat[indices[int(pct_train*N):int(pct_val*N)]]\n",
        "val_targets = targets[indices[int(pct_train*N):int(pct_val*N)]]\n",
        "test_data = dat[indices[int(pct_val*N):int(pct_test*N)]]\n",
        "test_targets = targets[indices[int(pct_val*N):int(pct_test*N)]]\n",
        "mynn = myNeuralNetwork(train_data, train_targets, 100, lr=0.001)\n",
        "mynn.train(val_data, val_targets, val_epoch)\n",
        "preds = mynn.predict(test_data)\n",
        "print(np.sum(test_targets[range(np.shape(test_targets)[0]), preds])/np.shape(test_targets)[0])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ko7CS6nL6qF3",
        "outputId": "f1d5d586-694c-4714-f225-eb315ed4015a"
      },
      "execution_count": 359,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(33750, 33)\n",
            "Epoch 0\n",
            "Loss: 1.0977584\n",
            "Loss: 1.0970507\n",
            "Loss: 1.0970405\n",
            "Loss: 1.0961371\n",
            "Loss: 1.0962325\n",
            "Loss: 1.0894820\n",
            "Validation loss: 1.0926929\n",
            "\n",
            "Epoch 1\n",
            "Loss: 1.0849492\n",
            "Loss: 1.0769226\n",
            "Loss: 1.0975290\n",
            "Loss: 1.0896115\n",
            "Loss: 1.0646850\n",
            "Loss: 1.0631751\n",
            "Validation loss: 1.0744963\n",
            "\n",
            "Epoch 2\n",
            "Loss: 1.0597215\n",
            "Loss: 1.0832008\n",
            "Loss: 1.0860864\n",
            "Loss: 1.0722570\n",
            "Loss: 1.0289705\n",
            "Loss: 1.0299378\n",
            "Validation loss: 1.0600350\n",
            "\n",
            "Epoch 3\n",
            "Loss: 1.0602345\n",
            "Loss: 1.0835174\n",
            "Loss: 1.0717200\n",
            "Loss: 1.0624927\n",
            "Loss: 1.0313283\n",
            "Loss: 1.0233531\n",
            "Validation loss: 1.0575411\n",
            "\n",
            "Epoch 4\n",
            "Loss: 1.0619373\n",
            "Loss: 1.0831769\n",
            "Loss: 1.0725026\n",
            "Loss: 1.0569575\n",
            "Loss: 1.0324021\n",
            "Loss: 1.0211590\n",
            "Validation loss: 1.0562630\n",
            "\n",
            "Epoch 5\n",
            "Loss: 1.0627320\n",
            "Loss: 1.0835694\n",
            "Loss: 1.0723024\n",
            "Loss: 1.0549647\n",
            "Loss: 1.0332197\n",
            "Loss: 1.0190020\n",
            "Validation loss: 1.0552314\n",
            "\n",
            "Epoch 6\n",
            "Loss: 1.0638840\n",
            "Loss: 1.0845535\n",
            "Loss: 1.0734589\n",
            "Loss: 1.0528925\n",
            "Loss: 1.0314967\n",
            "Loss: 1.0172584\n",
            "Validation loss: 1.0541852\n",
            "\n",
            "Epoch 7\n",
            "Loss: 1.0642263\n",
            "Loss: 1.0834384\n",
            "Loss: 1.0749092\n",
            "Loss: 1.0506939\n",
            "Loss: 1.0330060\n",
            "Loss: 1.0127293\n",
            "Validation loss: 1.0530640\n",
            "\n",
            "Epoch 8\n",
            "Loss: 1.0638381\n",
            "Loss: 1.0826374\n",
            "Loss: 1.0729323\n",
            "Loss: 1.0494325\n",
            "Loss: 1.0294991\n",
            "Loss: 1.0115770\n",
            "Validation loss: 1.0521209\n",
            "\n",
            "Epoch 9\n",
            "Loss: 1.0631698\n",
            "Loss: 1.0840026\n",
            "Loss: 1.0690175\n",
            "Loss: 1.0517990\n",
            "Loss: 1.0320741\n",
            "Loss: 1.0113555\n",
            "Validation loss: 1.0515426\n",
            "\n",
            "Epoch 10\n",
            "Loss: 1.0630476\n",
            "Loss: 1.0859117\n",
            "Loss: 1.0704438\n",
            "Loss: 1.0513147\n",
            "Loss: 1.0294976\n",
            "Loss: 1.0165533\n",
            "Validation loss: 1.0511438\n",
            "\n",
            "Epoch 11\n",
            "Loss: 1.0630114\n",
            "Loss: 1.0859861\n",
            "Loss: 1.0676096\n",
            "Loss: 1.0491048\n",
            "Loss: 1.0282708\n",
            "Loss: 1.0169256\n",
            "Validation loss: 1.0508363\n",
            "\n",
            "Epoch 12\n",
            "Loss: 1.0621307\n",
            "Loss: 1.0867385\n",
            "Loss: 1.0650767\n",
            "Loss: 1.0454935\n",
            "Loss: 1.0265606\n",
            "Loss: 1.0167863\n",
            "Validation loss: 1.0506193\n",
            "\n",
            "Epoch 13\n",
            "Loss: 1.0598802\n",
            "Loss: 1.0858962\n",
            "Loss: 1.0657622\n",
            "Loss: 1.0458733\n",
            "Loss: 1.0240746\n",
            "Loss: 1.0192403\n",
            "Validation loss: 1.0504597\n",
            "\n",
            "Epoch 14\n",
            "Loss: 1.0598751\n",
            "Loss: 1.0836608\n",
            "Loss: 1.0622011\n",
            "Loss: 1.0459703\n",
            "Loss: 1.0210123\n",
            "Loss: 1.0146686\n",
            "Validation loss: 1.0504991\n",
            "[0 1 0 ... 2 1 2]\n",
            "0.4442030416748963\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "batch_size = np.shape(train_data)[1]\n",
        "epochs = 200\n",
        "learning_rate = 0.001\n",
        "valid_epoch = 1\n",
        "\n",
        "model = torchNeuralNetwork(np.shape(train_data)[1], 3)\n",
        "loss_fn = nn.BCELoss()\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
        "best_loss = 10\n",
        "\n",
        "for e in range(epochs):\n",
        "  model.train()\n",
        "  print(\"Epoch {}\".format(e))\n",
        "  for batch in range(np.shape(train_data)[0]//batch_size):\n",
        "    pred = model(torch.Tensor(train_data[batch*batch_size : (batch+1)*batch_size]))\n",
        "    loss = loss_fn(pred, torch.Tensor(train_targets[batch*batch_size : (batch+1)*batch_size]))\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "    optimizer.zero_grad()\n",
        "    if(batch % 100 == 0):\n",
        "      loss, current = loss.item(), batch * batch_size\n",
        "      print(f\"loss: {loss:>7f}  [{current:>5d}/{np.shape(train_targets)[0]:>5d}]\")\n",
        "  if(e % valid_epoch == 0):\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "      pred = model(torch.Tensor(val_data))\n",
        "      loss = loss_fn(pred, torch.Tensor(val_targets)[0])\n",
        "      print(\"Validation loss: {:.7f}\".format(loss.item()))\n",
        "      if loss.item() > best_loss:\n",
        "        break\n",
        "      best_loss = loss.item()\n",
        "  print()\n",
        "  # print(f\"loss: {loss:>7f}\")\n",
        "\n",
        "model.eval()\n",
        "with torch.no_grad():\n",
        "  pred = model(torch.Tensor(test_data))\n",
        "\n",
        "  print(pred)\n",
        "  print(np.sum(test_targets[range(np.shape(test_targets)[0]), np.argmax(np.array(pred), axis=1)])/np.shape(test_targets)[0])\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FX2UciBML2gN",
        "outputId": "f483b250-6b43-4272-d9a6-dedc71d0a2b9"
      },
      "execution_count": 227,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 0\n",
            "loss: 0.638477  [    0/16875]\n",
            "loss: 0.629087  [ 3400/16875]\n",
            "loss: 0.637131  [ 6800/16875]\n",
            "loss: 0.610273  [10200/16875]\n",
            "loss: 0.600855  [13600/16875]\n",
            "Validation loss: 0.6162647\n",
            "\n",
            "Epoch 1\n",
            "loss: 0.625421  [    0/16875]\n",
            "loss: 0.599668  [ 3400/16875]\n",
            "loss: 0.635381  [ 6800/16875]\n",
            "loss: 0.603334  [10200/16875]\n",
            "loss: 0.599370  [13600/16875]\n",
            "Validation loss: 0.6154556\n",
            "\n",
            "Epoch 2\n",
            "loss: 0.623961  [    0/16875]\n",
            "loss: 0.598230  [ 3400/16875]\n",
            "loss: 0.634221  [ 6800/16875]\n",
            "loss: 0.604294  [10200/16875]\n",
            "loss: 0.599079  [13600/16875]\n",
            "Validation loss: 0.6152219\n",
            "\n",
            "Epoch 3\n",
            "loss: 0.622742  [    0/16875]\n",
            "loss: 0.598075  [ 3400/16875]\n",
            "loss: 0.633393  [ 6800/16875]\n",
            "loss: 0.605098  [10200/16875]\n",
            "loss: 0.598907  [13600/16875]\n",
            "Validation loss: 0.6151251\n",
            "\n",
            "Epoch 4\n",
            "loss: 0.621953  [    0/16875]\n",
            "loss: 0.598074  [ 3400/16875]\n",
            "loss: 0.632903  [ 6800/16875]\n",
            "loss: 0.605742  [10200/16875]\n",
            "loss: 0.598800  [13600/16875]\n",
            "Validation loss: 0.6150464\n",
            "\n",
            "Epoch 5\n",
            "loss: 0.621337  [    0/16875]\n",
            "loss: 0.598134  [ 3400/16875]\n",
            "loss: 0.632704  [ 6800/16875]\n",
            "loss: 0.606277  [10200/16875]\n",
            "loss: 0.598723  [13600/16875]\n",
            "Validation loss: 0.6149700\n",
            "\n",
            "Epoch 6\n",
            "loss: 0.620826  [    0/16875]\n",
            "loss: 0.598229  [ 3400/16875]\n",
            "loss: 0.632768  [ 6800/16875]\n",
            "loss: 0.606698  [10200/16875]\n",
            "loss: 0.598635  [13600/16875]\n",
            "Validation loss: 0.6148875\n",
            "\n",
            "Epoch 7\n",
            "loss: 0.620419  [    0/16875]\n",
            "loss: 0.598357  [ 3400/16875]\n",
            "loss: 0.633087  [ 6800/16875]\n",
            "loss: 0.606998  [10200/16875]\n",
            "loss: 0.598503  [13600/16875]\n",
            "Validation loss: 0.6147594\n",
            "\n",
            "Epoch 8\n",
            "loss: 0.620142  [    0/16875]\n",
            "loss: 0.598514  [ 3400/16875]\n",
            "loss: 0.633654  [ 6800/16875]\n",
            "loss: 0.607183  [10200/16875]\n",
            "loss: 0.598297  [13600/16875]\n",
            "Validation loss: 0.6145974\n",
            "\n",
            "Epoch 9\n",
            "loss: 0.620024  [    0/16875]\n",
            "loss: 0.598692  [ 3400/16875]\n",
            "loss: 0.634437  [ 6800/16875]\n",
            "loss: 0.607282  [10200/16875]\n",
            "loss: 0.597992  [13600/16875]\n",
            "Validation loss: 0.6144064\n",
            "\n",
            "Epoch 10\n",
            "loss: 0.620085  [    0/16875]\n",
            "loss: 0.598905  [ 3400/16875]\n",
            "loss: 0.635338  [ 6800/16875]\n",
            "loss: 0.607336  [10200/16875]\n",
            "loss: 0.597564  [13600/16875]\n",
            "Validation loss: 0.6141778\n",
            "\n",
            "Epoch 11\n",
            "loss: 0.620317  [    0/16875]\n",
            "loss: 0.599225  [ 3400/16875]\n",
            "loss: 0.636166  [ 6800/16875]\n",
            "loss: 0.607346  [10200/16875]\n",
            "loss: 0.596998  [13600/16875]\n",
            "Validation loss: 0.6139368\n",
            "\n",
            "Epoch 12\n",
            "loss: 0.620657  [    0/16875]\n",
            "loss: 0.599759  [ 3400/16875]\n",
            "loss: 0.636669  [ 6800/16875]\n",
            "loss: 0.607259  [10200/16875]\n",
            "loss: 0.596311  [13600/16875]\n",
            "Validation loss: 0.6136975\n",
            "\n",
            "Epoch 13\n",
            "loss: 0.620978  [    0/16875]\n",
            "loss: 0.600586  [ 3400/16875]\n",
            "loss: 0.636648  [ 6800/16875]\n",
            "loss: 0.607008  [10200/16875]\n",
            "loss: 0.595533  [13600/16875]\n",
            "Validation loss: 0.6135523\n",
            "\n",
            "Epoch 14\n",
            "loss: 0.621127  [    0/16875]\n",
            "loss: 0.601700  [ 3400/16875]\n",
            "loss: 0.636083  [ 6800/16875]\n",
            "loss: 0.606572  [10200/16875]\n",
            "loss: 0.594673  [13600/16875]\n",
            "Validation loss: 0.6134698\n",
            "\n",
            "Epoch 15\n",
            "loss: 0.621010  [    0/16875]\n",
            "loss: 0.603019  [ 3400/16875]\n",
            "loss: 0.635117  [ 6800/16875]\n",
            "loss: 0.605982  [10200/16875]\n",
            "loss: 0.593707  [13600/16875]\n",
            "Validation loss: 0.6134348\n",
            "\n",
            "Epoch 16\n",
            "loss: 0.620597  [    0/16875]\n",
            "loss: 0.604427  [ 3400/16875]\n",
            "loss: 0.633943  [ 6800/16875]\n",
            "loss: 0.605290  [10200/16875]\n",
            "loss: 0.592616  [13600/16875]\n",
            "Validation loss: 0.6134483\n",
            "tensor([[0.2376, 0.3419, 0.4206],\n",
            "        [0.1945, 0.2085, 0.5969],\n",
            "        [0.4462, 0.3996, 0.1541],\n",
            "        ...,\n",
            "        [0.4498, 0.3146, 0.2357],\n",
            "        [0.3025, 0.2626, 0.4349],\n",
            "        [0.3510, 0.4352, 0.2138]])\n",
            "0.43729014418329054\n"
          ]
        }
      ]
    }
  ]
}