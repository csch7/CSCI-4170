{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOcDgYkMA57mnMVY4Vf8DZl",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/csch7/CSCI-4170/blob/main/Homework-05/NLP_and_Attention.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "quFtVxPPBMLP"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import torch\n",
        "from torch import nn"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def softmax(scores):\n",
        "  return torch.exp(scores) / torch.sum(torch.exp(scores), dim = 0)\n",
        "\n",
        "def scaled_dot_product_attention(queries, keys, values):\n",
        "  queries = torch.unsqueeze(torch.unsqueeze(queries, 1), 2).repeat((1,1,64))\n",
        "  scores = (queries @ keys.permute(1,2,0)) / np.sqrt(keys.shape[-1])\n",
        "  s = softmax(scores)\n",
        "  return torch.squeeze(s @ values.permute(1,0,2), 1)\n",
        "\n",
        "\n",
        "# class DotProductAttention(nn.Layer):\n",
        "#   def __init__(self):\n",
        "#     super().__init__()\n",
        "\n",
        "#   def call(self, queries, keys, values):\n"
      ],
      "metadata": {
        "id": "IKCGMAcevO22"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Encoder(nn.Module):\n",
        "  def __init__(self, input_dim, embed_dim, hidden_dim):\n",
        "    super().__init__()\n",
        "    self.embed = nn.Embedding(input_dim, embed_dim)\n",
        "    self.lstm = nn.LSTM(embed_dim, hidden_dim, bidirectional=True)\n",
        "    self.fc = nn.Linear(2*hidden_dim, hidden_dim)\n",
        "    self.tanh = nn.Tanh()\n",
        "\n",
        "  def forward(self, x):\n",
        "    em = self.embed(x)\n",
        "    lstm_out, (hidden, _) = self.lstm(em)\n",
        "    return lstm_out, self.tanh(self.fc(torch.cat((hidden[0], hidden[1]), dim=1)))\n",
        "\n",
        "class Decoder(nn.Module):\n",
        "  def __init__(self, embed_dim, hidden_dim, output_dim):\n",
        "    super().__init__()\n",
        "    self.embed = nn.Embedding(output_dim, embed_dim)\n",
        "    self.lstm = nn.LSTM(2*hidden_dim + embed_dim, hidden_dim)\n",
        "    self.fc = nn.Linear(hidden_dim, output_dim)\n",
        "\n",
        "  def forward(self, targets, hidden, encoder_out):\n",
        "    em = self.embed(targets)\n",
        "    # print(hidden.shape)\n",
        "    attn = scaled_dot_product_attention(hidden[:,-1], encoder_out, encoder_out)\n",
        "    lstm_out, (hidden, _) = self.lstm(torch.cat((em, attn), dim=1))\n",
        "    return self.fc(lstm_out), hidden\n",
        "\n",
        "class Seq2SeqAttn(nn.Module):\n",
        "  def __init__(self, enc, dec, out_vocab_len):\n",
        "    super().__init__()\n",
        "    self.encoder = enc\n",
        "    self.decoder = dec\n",
        "    self.vocab_len = out_vocab_len\n",
        "\n",
        "  def forward(self, inputs, targets):\n",
        "    tar_len = targets.shape[0]\n",
        "    tar_size = targets.shape[1]\n",
        "    outputs = torch.zeros((tar_len, tar_size, self.vocab_len))\n",
        "    enc_out, hidden = self.encoder(inputs)\n",
        "    for i in range(1, tar_len):\n",
        "      dec_out, hidden = self.decoder(targets[i], hidden, enc_out)\n",
        "      outputs[i] = dec_out\n",
        "    return outputs\n"
      ],
      "metadata": {
        "id": "clSrKOutXJPw"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install datasets"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IcBpND-_kc8-",
        "outputId": "3a1c956f-0d98-4dc8-aba0-3ab310ecf226"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: datasets in /usr/local/lib/python3.11/dist-packages (3.4.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from datasets) (3.17.0)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from datasets) (1.26.4)\n",
            "Requirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.11/dist-packages (from datasets) (18.1.0)\n",
            "Requirement already satisfied: dill<0.3.9,>=0.3.0 in /usr/local/lib/python3.11/dist-packages (from datasets) (0.3.8)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (from datasets) (2.2.2)\n",
            "Requirement already satisfied: requests>=2.32.2 in /usr/local/lib/python3.11/dist-packages (from datasets) (2.32.3)\n",
            "Requirement already satisfied: tqdm>=4.66.3 in /usr/local/lib/python3.11/dist-packages (from datasets) (4.67.1)\n",
            "Requirement already satisfied: xxhash in /usr/local/lib/python3.11/dist-packages (from datasets) (3.5.0)\n",
            "Requirement already satisfied: multiprocess<0.70.17 in /usr/local/lib/python3.11/dist-packages (from datasets) (0.70.16)\n",
            "Requirement already satisfied: fsspec<=2024.12.0,>=2023.1.0 in /usr/local/lib/python3.11/dist-packages (from fsspec[http]<=2024.12.0,>=2023.1.0->datasets) (2024.10.0)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.11/dist-packages (from datasets) (3.11.13)\n",
            "Requirement already satisfied: huggingface-hub>=0.24.0 in /usr/local/lib/python3.11/dist-packages (from datasets) (0.28.1)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from datasets) (24.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from datasets) (6.0.2)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (2.5.0)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (1.3.2)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (25.1.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (1.5.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (6.1.0)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (0.3.0)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (1.18.3)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.24.0->datasets) (4.12.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets) (2025.1.31)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets) (2025.1)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets) (2025.1)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.17.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from datasets import load_dataset\n",
        "\n",
        "ds = load_dataset('bentrevett/multi30k')\n",
        "\n",
        "train_dat = ds['train'][:len(ds['train'])//20]\n",
        "valid_dat = ds['validation'][:len(ds['validation'])//20]\n",
        "test_dat = ds['test'][:len(ds['test'])//20]\n",
        "train_lab = train_dat['en']\n",
        "train_dat = train_dat['de']\n",
        "valid_lab = valid_dat['en']\n",
        "valid_dat = valid_dat['de']\n",
        "test_lab = test_dat['en']\n",
        "test_dat = test_dat['de']"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nk2uHcBlkfkx",
        "outputId": "f699231d-19e0-4f25-d72e-1ba7471c917b"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "\n",
        "def clean_text(text):\n",
        "    text = str(text).lower() # Ensure no duplicate word embeddings due to capital letters\n",
        "    test = re.sub(r'^[A-Za-zÀ-ȕ ]+', '', text)         # Remove certain special characters (need to be careful not to remove umlauds or eszetts from German)\n",
        "    text = re.sub(r\"\\s+\", \" \", text).strip()      # Remove extra spaces\n",
        "    return text\n",
        "\n",
        "def pad_sentences(dat, max_len):\n",
        "  for s in range(len(dat)):\n",
        "    if len(dat[s]) > max_len:\n",
        "      dat[s] = dat[s][:max_len]\n",
        "    else:\n",
        "      dat[s] = dat[s] + ['<PAD>']*(max_len-len(dat[s]))\n",
        "  return dat\n",
        "\n",
        "def process_sentences(dat, vocab, max_len):\n",
        "  dat = ['<SOS> '+s+' <EOS>' for s in dat]\n",
        "  dat = [s.split() for s in dat]\n",
        "  dat = pad_sentences(dat, max_len)\n",
        "  dat = [[vocab[word] for word in s] for s in dat]\n",
        "  return dat\n",
        "\n",
        "\n",
        "max_len = 50\n",
        "\n",
        "sentences_en = ['<SOS> '+s+' <EOS>' for ds in [train_lab, valid_lab, test_lab] for s in ds]\n",
        "sentences_en = [s.split() for s in sentences_en]\n",
        "vocab_en = set([w for s in sentences_en for w in s])\n",
        "vocab_en = {word: idx+1 for idx, word in enumerate(vocab_en)}\n",
        "vocab_en['<PAD>'] = 0\n",
        "token_to_value_en = {vocab_en[k]: k for k in vocab_en}\n",
        "\n",
        "sentences_de = ['<SOS> '+s+' <EOS>' for ds in [train_dat, valid_dat, test_dat] for s in ds]\n",
        "sentences_de = [s.split() for s in sentences_de]\n",
        "vocab_de = set([w for s in sentences_de for w in s])\n",
        "vocab_de = {word: idx+1 for idx, word in enumerate(vocab_de)}\n",
        "vocab_de['<PAD>'] = 0\n",
        "token_to_value_de = {vocab_de[k]: k for k in vocab_de}\n",
        "\n",
        "train_dat = process_sentences(train_dat, vocab_de, max_len)\n",
        "train_lab = process_sentences(train_lab, vocab_en, max_len)\n",
        "valid_dat = process_sentences(valid_dat, vocab_de, max_len)\n",
        "valid_lab = process_sentences(valid_lab, vocab_en, max_len)\n",
        "test_dat = process_sentences(test_dat, vocab_de, max_len)\n",
        "test_lab = process_sentences(test_lab, vocab_en, max_len)"
      ],
      "metadata": {
        "id": "m0Z-Wf9llXcC"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch.optim as optim\n",
        "\n",
        "def one_hot_encode(labels, max_len, vocab_size):\n",
        "  res = torch.zeros((len(labels), max_len, vocab_size))\n",
        "  for i in range(len(labels)):\n",
        "    for j in range(max_len):\n",
        "      res[i,j,labels[i,j]] = 1\n",
        "  return res\n",
        "\n",
        "\n",
        "train_dat = torch.LongTensor(train_dat)\n",
        "train_lab = torch.LongTensor(train_lab)\n",
        "train_ohe = one_hot_encode(train_lab, max_len, len(vocab_en))\n",
        "valid_dat = torch.LongTensor(valid_dat)\n",
        "valid_lab = torch.LongTensor(valid_lab)\n",
        "valid_ohe = one_hot_encode(valid_lab, max_len, len(vocab_en))\n",
        "test_dat = torch.LongTensor(test_dat)\n",
        "test_lab = torch.LongTensor(test_lab)\n",
        "test_ohe = one_hot_encode(test_lab, max_len, len(vocab_en))\n",
        "\n",
        "epochs = 10\n",
        "enc = Encoder(len(vocab_de), 100, 32)\n",
        "dec = Decoder(100, 32, len(vocab_en))\n",
        "model = Seq2SeqAttn(enc, dec, len(vocab_en))\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
        "loss_fn = nn.CrossEntropyLoss()\n",
        "\n",
        "for e in range(epochs):\n",
        "  model.train()\n",
        "  optimizer.zero_grad()\n",
        "  pred = model(train_dat.T, train_lab.T)\n",
        "  pred = torch.permute(pred, (1,0,2))\n",
        "  print([token_to_value_en[int(w)] for w in train_lab[0]], [token_to_value_en[int(w)] for w in torch.argmax(pred[0], dim=1)])\n",
        "  loss = loss_fn(pred, train_ohe)\n",
        "  print(loss.item())\n",
        "  loss.backward()\n",
        "  optimizer.step()\n",
        "\n",
        "  model.eval()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KiB0Iu_v5P9j",
        "outputId": "812e6501-9be2-4329-95ad-835c312ef499"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['<SOS>', 'Two', 'young,', 'White', 'males', 'are', 'outside', 'near', 'many', 'bushes.', '<EOS>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>'] ['<PAD>', 'smile.', 'volleyball.', 'middle.', 'jump.', 'jumpsuit', 'mobile', 'pot', 'cowboy', 'splashing', 'airport.', 'two', 'two', 'two', 'two', 'two', 'two', 'two', 'two', 'two', 'two', 'two', 'two', 'two', 'two', 'two', 'two', 'two', 'two', 'two', 'two', 'two', 'two', 'two', 'two', 'two', 'two', 'two', 'two', 'two', 'two', 'two', 'two', 'two', 'two', 'two', 'two', 'two', 'two', 'two']\n",
            "0.06465520709753036\n",
            "['<SOS>', 'Two', 'young,', 'White', 'males', 'are', 'outside', 'near', 'many', 'bushes.', '<EOS>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>'] ['<PAD>', 'shaggy', 'volleyball.', 'middle.', 'jump.', 'jumpsuit', 'mobile', 'pot', 'cowboy', 'splashing', 'airport.', 'bib', 'bib', 'bib', 'bib', 'bib', 'bib', 'bib', 'bib', 'bib', 'bib', 'bib', 'bib', 'bib', 'bib', 'bib', 'bib', 'bib', 'bib', 'bib', 'bib', 'bib', 'bib', 'bib', 'bib', 'bib', 'bib', 'bib', 'bib', 'bib', 'bib', 'bib', 'bib', 'bib', 'bib', 'bib', 'bib', 'bib', 'bib', 'bib']\n",
            "0.06421832740306854\n",
            "['<SOS>', 'Two', 'young,', 'White', 'males', 'are', 'outside', 'near', 'many', 'bushes.', '<EOS>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>'] ['<PAD>', 'shaggy', 'volleyball.', 'middle.', 'jump.', 'jumpsuit', 'mobile', 'pot', 'cowboy', 'splashing', 'stances,', 'bib', 'bib', 'bib', 'bib', 'bib', 'bib', 'bib', 'bib', 'bib', 'bib', 'bib', 'bib', 'bib', 'bib', 'bib', 'bib', 'bib', 'bib', 'bib', 'bib', 'bib', 'bib', 'bib', 'bib', 'bib', 'bib', 'bib', 'bib', 'bib', 'bib', 'bib', 'bib', 'bib', 'bib', 'bib', 'bib', 'bib', 'bib', 'bib']\n",
            "0.06383636593818665\n",
            "['<SOS>', 'Two', 'young,', 'White', 'males', 'are', 'outside', 'near', 'many', 'bushes.', '<EOS>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>'] ['<PAD>', 'shaggy', 'volleyball.', 'middle.', 'jump.', 'jumpsuit', 'jump.', 'pot', 'cowboy', 'splashing', 'stances,', 'bib', 'bib', 'bib', 'bib', 'bib', 'bib', 'bib', 'bib', 'bib', 'bib', 'bib', 'bib', 'bib', 'bib', 'bib', 'bib', 'bib', 'bib', 'bib', 'bib', 'bib', 'bib', 'bib', 'bib', 'bib', 'bib', 'bib', 'bib', 'bib', 'bib', 'bib', 'bib', 'bib', 'bib', 'bib', 'bib', 'bib', 'bib', 'bib']\n",
            "0.06349895894527435\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "_\n",
        "class PositionalEncoding(nn.Module):\n",
        "  def __init__(self, seq_len, embedding_dim):\n",
        "    super().__init__()\n",
        "    self.seq_len = seq_len\n",
        "    self.embed_dim = embedding_dim\n",
        "\n",
        "  def forward(self, x):\n",
        "    embedding = torch.zeros((self.seq_len, self.embed_dim))\n",
        "    positions = torch.arange(self.seq_len)\n",
        "    embedding[positions, ::2] = torch.sin(positions*(10000**(2*torch.arange(self.embed_dim)[:self.embed_dim//2]/self.embed_dim)))\n",
        "    embedding[positions, 1::2] = torch.cos(positions*(10000**(2*torch.arange(self.embed_dim)[:self.embed_dim//2]/self.embed_dim)))\n",
        "    return embedding\n",
        "\n",
        "\n",
        "class ScaledDotProductAttention(nn.Module):\n",
        "  def __init__(self):\n",
        "    super().__init__()\n",
        "    self.sm = nn.Softmax(dim=0)\n",
        "\n",
        "  def forward(self, q, k, v):\n",
        "    scores = np.matmul(q, k.T) / np.sqrt(np.shape(k)[-1])\n",
        "    s = softmax(scores)\n",
        "    return np.matmul(s, v)\n",
        "\n",
        "class MultiHeadAttention(nn.Module):\n",
        "  def __init__(self, num_heads, qk, qv, dim_model):\n",
        "    self.nh = num_heads\n",
        "    self.d_model = dim_model\n",
        "    self.Wq = nn.Parameter(torch.randn(num_heads, dim_model, qk))\n",
        "    self.Wk = nn.Parameter(torch.randn(num_heads, dim_model, qk))\n",
        "    self.Wv = nn.Parameter(torch.randn(num_heads, dim_model, qv))\n",
        "    self.Wo = nn.Parameter(torch.randn(num_heads*qv, dim_model))\n",
        "    self.attn = ScaledDotProductAttention()\n",
        "\n",
        "  def forward(self, Q, K, V):\n",
        "    heads = self.attn(Q @ self.Wq, K @ self.Wk, V @ self.Wv)\n",
        "    output = heads[0]\n",
        "    for i in range(1, self.nh):\n",
        "      output = torch.cat(output, heads[i])\n",
        "\n",
        "\n",
        "class FFN(nn.Module):\n",
        "  def __init__(self, embedding_dim = 64, hidden_dim = 128):\n",
        "    super().__init__()\n",
        "    self.w1 = nn.Parameter(torch.randn(embedding_dim, hidden_dim))\n",
        "    self.b1 = nn.Parameter(torch.randn(hidden_dim))\n",
        "    self.w2 = nn.Parameter(torch.randn(hidden_dim, embedding_dim))\n",
        "    self.b2 = nn.Parameter(torch.randn(embedding_dim))\n",
        "\n",
        "  def forward(self, x):\n",
        "    return (torch.max(0, (x @ self.w1) + self.b1) @ self.w2) + self.b2\n",
        "\n",
        "\n",
        "class Encoder(nn.Module):\n",
        "  def __init__(self, input_len, vocab_size, d_model = 64, hidden_dim = 128, num_heads = 8, num_layers = 2):\n",
        "    super().__init__()\n",
        "    self.d_model = d_model\n",
        "    self.hidden_dim = hidden_dim\n",
        "    self.embed = nn.Embedding(vocab_size, d_model)\n",
        "    self.position = PositionalEncoding(input_len, d_model)\n",
        "    self.ffn = FFN(d_model, hidden_dim)\n",
        "    self.layernorm = nn.LayerNorm((input_len, d_model))\n",
        "    self.attn = MultiHeadAttention(num_heads, d_model / num_heads, d_model / num_heads, d_model)\n",
        "    self.L = num_layers\n",
        "\n",
        "  def forward(self, inputs):\n",
        "    em = self.embed(inputs)\n",
        "    pos_en = self.position(inputs)\n",
        "    out = em + pos_en\n",
        "    for l in range(self.L):\n",
        "      self_attn = self.attn(out, out, out)\n",
        "      attn_norm = self.layernorm(self_attn + out)\n",
        "      ffn_out = self.FFN(attn_norm)\n",
        "      out = self.layernorm(ffn_out + attn_norm)\n",
        "    return out\n",
        "\n",
        "\n",
        "class Decoder(nn.Module):\n",
        "  def __init__(self, output_len, vocab_size, d_model = 64, hidden_dim = 128, num_heads = 8, num_layers = 2):\n",
        "    super().__init__()\n",
        "    self.d_model = d_model\n",
        "    self.hidden_dim = hidden_dim\n",
        "    self.embed = nn.Embedding(vocab_size, d_model)\n",
        "    self.position = PositionalEncoding(output_len, d_model)\n",
        "    self.ffn = FFN(d_model, hidden_dim)\n",
        "    self.layernorm = nn.LayerNorm((output_len, d_model))\n",
        "    self.attn = MultiHeadAttention(num_heads, d_model / num_heads, d_model / num_heads, d_model)\n",
        "    self.masked_attn = MultiHeadAttention(num_heads, d_model / num_heads, d_model / num_heads, d_model) # FIGURE OUT MASK\n",
        "    self.L = num_layers\n",
        "\n",
        "  def forward(self, outputs, enc_out):\n",
        "    em = self.embed(outputs)\n",
        "    pos_en = self.position(outputs)\n",
        "    out = em + pos_en\n",
        "    for l in range(self.L):\n",
        "      self_attn = self.masked_attn(out, out, out)\n",
        "      attn_norm = self.layernorm(self_attn + out)\n",
        "      enc_attn = self.attn(enc_out, enc_out, attn_norm)\n",
        "      attn_norm = self.layernorm(enc_attn + attn_norm)\n",
        "      ffn_out = self.FFN(attn_norm)\n",
        "      out = self.layernorm(ffn_out + attn_norm)\n",
        "    return out\n",
        "\n",
        "\n",
        "class Transformer(nn.Module):\n",
        "  def __init__(self, input_len, output_len, vocab_size, d_model = 64, hidden_dim = 128, num_heads = 8, num_layers = 2):\n",
        "    super().__init__()\n",
        "    self.encoder = Encoder(input_len)\n",
        "    self.decoder = Decoder(output_len)\n",
        "    self.fc = nn.Linear(d_model, vocab_size)\n",
        "    self.sm = nn.Softmax(dim=0)\n",
        "\n",
        "  def forward(self, inputs, outputs):\n",
        "    enc_out = self.encoder(inputs)\n",
        "    dec_out = self.decoder(outputs, enc_out)\n",
        "    return self.sm(self.fc(dec_out))"
      ],
      "metadata": {
        "id": "8HxBa9HVQylR"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}